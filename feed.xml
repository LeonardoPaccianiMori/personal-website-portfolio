<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://unique-cocada-065d88.netlify.app//feed.xml" rel="self" type="application/atom+xml"/><link href="https://unique-cocada-065d88.netlify.app//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-24T16:15:22+00:00</updated><id>https://unique-cocada-065d88.netlify.app//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building an LLM-Powered Insights Engine That Doesn’t Hallucinate</title><link href="https://unique-cocada-065d88.netlify.app//blog/2025/llm-insights-engine/" rel="alternate" type="text/html" title="Building an LLM-Powered Insights Engine That Doesn’t Hallucinate"/><published>2025-10-24T10:00:00+00:00</published><updated>2025-10-24T10:00:00+00:00</updated><id>https://unique-cocada-065d88.netlify.app//blog/2025/llm-insights-engine</id><content type="html" xml:base="https://unique-cocada-065d88.netlify.app//blog/2025/llm-insights-engine/"><![CDATA[<h2 id="the-problem-insights-buried-in-data-silos">The Problem: Insights Buried in Data Silos</h2> <p>When I joined my current company as a Senior Data Scientist, our audience insights process looked like this:</p> <ol> <li>An analyst receives a client request for audience insights</li> <li>They manually query <strong>one data source</strong> (maybe attitudinal surveys, maybe behavioral data)</li> <li>They create charts and write commentary in PowerPoint</li> <li>Repeat for each additional data source the client wants</li> <li>Deliver report weeks later</li> </ol> <p><strong>The issues were obvious:</strong></p> <ul> <li><strong>Time-consuming</strong>: Each report took days or weeks of manual work</li> <li><strong>Single-source bias</strong>: Using one data source at a time meant missing the full picture</li> <li><strong>Not scalable</strong>: Each new client request started from scratch</li> <li><strong>Limited interactivity</strong>: Clients couldn’t ask follow-up questions without another round of analysis</li> </ul> <p>The data was all there—attitudinal surveys, brand perception studies, behavioral data, location data—but it lived in separate silos. And even when analysts combined sources, the sheer volume made it overwhelming to navigate and extract meaningful insights.</p> <p><strong>What we needed:</strong> A system that could integrate multiple data sources and help users discover insights without requiring a data scientist for every question.</p> <p><strong>The natural solution in 2025:</strong> Large Language Models.</p> <hr/> <h2 id="the-vision-multi-source-insights-on-demand">The Vision: Multi-Source Insights on Demand</h2> <p>I proposed building a system that would:</p> <ol> <li><strong>Integrate multiple data sources</strong> into a unified view</li> <li><strong>Use LLMs</strong> to automatically generate insights from the combined data</li> <li><strong>Provide an interactive chat interface</strong> where users could ask questions naturally</li> <li><strong>Deliver accurate, reliable answers</strong> suitable for client-facing work</li> </ol> <p>The last point was critical. This wasn’t an internal tool where “mostly right” would be acceptable. These insights would go directly to paying clients. <strong>Hallucinations were a dealbreaker.</strong></p> <hr/> <h2 id="the-challenge-llms-hallucinate-a-lot">The Challenge: LLMs Hallucinate (A Lot)</h2> <p>If you’ve worked with LLMs, you know the problem: they’re <em>incredibly</em> good at generating plausible-sounding text, even when it’s completely wrong.</p> <p>Ask an LLM about your data, and you might get:</p> <ul> <li>Statistics that don’t exist in your dataset</li> <li>Trends that are pure invention</li> <li>Confident assertions based on nothing</li> </ul> <p>For internal exploration, you can fact-check. For client deliverables, <strong>you need zero tolerance for hallucination.</strong></p> <h3 id="why-standard-rag-wasnt-enough">Why Standard RAG Wasn’t Enough</h3> <p>The standard approach for grounding LLMs is Retrieval-Augmented Generation (RAG): retrieve relevant context, pass it to the LLM, generate response. But in my case:</p> <ul> <li><strong>Problem 1</strong>: [Describe specific limitation of RAG for your use case]</li> <li><strong>Problem 2</strong>: [Another challenge you faced]</li> <li><strong>Problem 3</strong>: [Why it wasn’t sufficient for your data]</li> </ul> <p>I needed something more robust.</p> <hr/> <h2 id="the-solution-your-anti-hallucination-approach">The Solution: [Your Anti-Hallucination Approach]</h2> <p>After experimenting with several approaches, I developed a system that <strong>essentially eliminates hallucinations</strong> through [describe your key innovation].</p> <h3 id="the-key-insight">The Key Insight</h3> <p>[Explain the conceptual breakthrough that made your solution work. What did you realize about the problem that led to your approach?]</p> <h3 id="technical-implementation">Technical Implementation</h3> <p><strong>Architecture Overview:</strong></p> <p>The system has [X] main components:</p> <ol> <li><strong>Data Integration Layer</strong> <ul> <li>[How you unified the different data sources]</li> <li>[What transformations or standardizations were needed]</li> <li>[Technologies used: databases, APIs, etc.]</li> </ul> </li> <li><strong>Query Processing</strong> <ul> <li>[How you parse user questions]</li> <li>[How you determine what data is relevant]</li> <li>[Any NLP preprocessing steps]</li> </ul> </li> <li><strong>LLM Orchestration</strong> <ul> <li><strong>Model used</strong>: [Which LLM? OpenAI, Anthropic, open-source?]</li> <li><strong>Prompting strategy</strong>: [How you structured prompts]</li> <li><strong>Temperature settings</strong>: [What temperature/sampling you used]</li> </ul> </li> <li><strong>Hallucination Prevention</strong> <ul> <li>[Your specific technique - this is the most important part!]</li> <li>[How you verify LLM outputs against actual data]</li> <li>[Any validation or checking mechanisms]</li> </ul> </li> <li><strong>Response Generation</strong> <ul> <li>[How you format final outputs]</li> <li>[Any post-processing steps]</li> </ul> </li> </ol> <h3 id="why-this-works">Why This Works</h3> <p>[Explain why your approach prevents hallucinations. What’s the mechanism that ensures accuracy?]</p> <p><strong>Example workflow:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User asks: "What are the top interests of our audience in the 25-34 age group?"

System does:
1. [Step 1 of your process]
2. [Step 2]
3. [Step 3]
4. [Final validation step that ensures accuracy]

Output: [Example of what the system returns]
</code></pre></div></div> <hr/> <h2 id="implementation-why-streamlit-not-tableau">Implementation: Why Streamlit (Not Tableau)</h2> <p>For the interface, I chose <strong>Streamlit</strong> over traditional BI tools like Tableau because:</p> <ol> <li><strong>Custom LLM integration</strong>: [Why Tableau couldn’t handle your LLM needs]</li> <li><strong>Interactive chat interface</strong>: [What you needed that BI tools don’t provide]</li> <li><strong>[Other reason]</strong>: [Explain]</li> </ol> <h3 id="the-tech-stack">The Tech Stack</h3> <ul> <li><strong>Frontend</strong>: Streamlit</li> <li><strong>Backend</strong>: [Python? FastAPI? Other?]</li> <li><strong>LLM Integration</strong>: [How you connected to the LLM - API, local model?]</li> <li><strong>Data Storage</strong>: [Where the data lives - databases, data warehouse?]</li> <li><strong>Data Sources</strong>: <ul> <li>Attitudinal surveys</li> <li>Brand perception data</li> <li>Behavioral data</li> <li>Location data</li> <li>[Any others?]</li> </ul> </li> </ul> <h3 id="user-experience">User Experience</h3> <p>The interface is deliberately simple:</p> <ol> <li><strong>Data source selector</strong>: Users choose which datasets to include</li> <li><strong>Chat interface</strong>: Natural language questions</li> <li><strong>Insight display</strong>: [How you present results - charts, tables, text?]</li> <li><strong>[Other features]</strong>: [Anything else worth highlighting?]</li> </ol> <hr/> <h2 id="the-results-client-feedback">The Results: Client Feedback</h2> <p>When we demoed the system to potential clients, the feedback was <strong>overwhelmingly enthusiastic</strong>.</p> <p><strong>What resonated most:</strong></p> <ul> <li><strong>Speed</strong>: Insights that used to take days now available instantly</li> <li><strong>Depth</strong>: Multi-source integration revealed patterns that single-source analysis missed</li> <li><strong>Interactivity</strong>: Clients could explore their data conversationally</li> <li><strong>Trust</strong>: [How did you demonstrate the accuracy? What built their confidence?]</li> </ul> <h3 id="quantitative-impact">Quantitative Impact</h3> <p>Compared to the old manual process:</p> <ul> <li><strong>Time savings</strong>: [X hours/days saved per report?]</li> <li><strong>Data coverage</strong>: [X data sources vs 1 before?]</li> <li><strong>Client satisfaction</strong>: [Any NPS scores or feedback metrics?]</li> <li><strong>Business impact</strong>: [New deals closed? Existing clients expanded usage?]</li> </ul> <hr/> <h2 id="lessons-learned">Lessons Learned</h2> <h3 id="1-hallucination-prevention-is-non-negotiable-for-production">1. Hallucination Prevention is Non-Negotiable for Production</h3> <p>You can’t just hope the LLM gets it right. You need <strong>systematic verification</strong> at every step.</p> <p>[Expand on this based on your experience]</p> <h3 id="2-second-major-lesson">2. [Second Major Lesson]</h3> <p>[What else did you learn? About LLMs, about your data, about user needs?]</p> <h3 id="3-third-major-lesson">3. [Third Major Lesson]</h3> <p>[Another key takeaway from the project]</p> <h3 id="4-the-right-tool-for-the-job">4. The Right Tool for the Job</h3> <p>Sometimes the “obvious” tool (Tableau for BI) isn’t the right choice when you’re doing something novel. Don’t be afraid to pick the technology that actually fits your requirements.</p> <hr/> <h2 id="whats-next">What’s Next</h2> <p>The system is currently [in production / in pilot / being expanded / other status]. Next steps include:</p> <ul> <li>[Future enhancement 1]</li> <li>[Future enhancement 2]</li> <li>[Future enhancement 3]</li> </ul> <hr/> <h2 id="technical-deep-dive-specific-component">Technical Deep Dive: [Specific Component]</h2> <details> <summary><strong>Click to expand: [Topic]</strong></summary> [Detailed technical explanation of a specific component, algorithm, or technique] </details> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <p>For anyone building LLM-powered analytics tools:</p> <ol> <li><strong>Hallucination prevention must be built into the architecture</strong>, not bolted on later</li> <li><strong>Multi-source data integration is powerful</strong> but requires careful schema design</li> <li><strong>User trust is earned through transparency</strong> - [how did you build trust?]</li> <li><strong>[Your fourth takeaway]</strong></li> </ol> <p>The opportunity with LLMs isn’t to replace human analysts—it’s to <strong>amplify their capabilities</strong> and make insights accessible to more people. But only if you can guarantee accuracy.</p> <hr/> <h2 id="technologies-used">Technologies Used</h2> <table> <thead> <tr> <th><strong>Category</strong></th> <th><strong>Technology</strong></th> </tr> </thead> <tbody> <tr> <td>LLM</td> <td>[Model name]</td> </tr> <tr> <td>Frontend</td> <td>Streamlit</td> </tr> <tr> <td>Backend</td> <td>[Your stack]</td> </tr> <tr> <td>Data Integration</td> <td>[Tools/DBs used]</td> </tr> <tr> <td>Data Storage</td> <td>[Databases]</td> </tr> <tr> <td>Deployment</td> <td>[How/where deployed]</td> </tr> </tbody> </table> <hr/> <h2 id="related-posts">Related Posts</h2> <ul> <li>[Future post on synthetic data for real estate]</li> <li>[Future post on GAN debugging]</li> </ul> <hr/> <p><strong>Questions or comments?</strong> I’d love to hear from other data scientists working on production LLM systems. What approaches have you found effective for hallucination prevention?</p> ]]></content><author><name></name></author><category term="data-science"/><category term="LLM"/><category term="machine-learning"/><category term="data-engineering"/><summary type="html"><![CDATA[How I redesigned our audience insights system using LLMs while solving the hallucination problem for client-facing work]]></summary></entry><entry><title type="html">From MongoDB to PostgreSQL: Database Architecture for ML Projects</title><link href="https://unique-cocada-065d88.netlify.app//blog/2025/mongodb-postgresql-ml/" rel="alternate" type="text/html" title="From MongoDB to PostgreSQL: Database Architecture for ML Projects"/><published>2025-06-20T10:30:00+00:00</published><updated>2025-06-20T10:30:00+00:00</updated><id>https://unique-cocada-065d88.netlify.app//blog/2025/mongodb-postgresql-ml</id><content type="html" xml:base="https://unique-cocada-065d88.netlify.app//blog/2025/mongodb-postgresql-ml/"><![CDATA[<h2 id="the-problem-from-raw-html-to-ml-ready-data">The Problem: From Raw HTML to ML-Ready Data</h2> <p>In my <a href="/projects/italian-real-estate/">Italian Real Estate project</a>, I needed to build a pipeline that transformed messy web data into clean, ML-ready datasets.</p> <p><strong>The journey:</strong></p> <ol> <li>Scrape 1M+ property listings from Italian real estate websites (raw HTML)</li> <li>Extract 50+ fields from unstructured HTML</li> <li>Translate Italian text to English</li> <li>Create a normalized, relational schema</li> <li>Train ML models to predict rental prices</li> </ol> <p><strong>The question:</strong> What database architecture supports this progression from chaos to structure?</p> <p><strong>The answer:</strong> Use <strong>both</strong> MongoDB (datalake + warehouse) and PostgreSQL (analytics-ready), with clear transitions between them.</p> <p>This post explains why I made this choice and how the multi-database architecture evolved.</p> <hr/> <h2 id="the-three-layer-architecture">The Three-Layer Architecture</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/italian-real-estate/italian-real-estate-project-structure-480.webp 480w,/assets/img/projects/italian-real-estate/italian-real-estate-project-structure-800.webp 800w,/assets/img/projects/italian-real-estate/italian-real-estate-project-structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/italian-real-estate/italian-real-estate-project-structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Data pipeline architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Three-layer architecture: MongoDB datalake → MongoDB warehouse → PostgreSQL analytics database </div> <h3 id="layer-1-mongodb-datalake-raw-html">Layer 1: MongoDB Datalake (Raw HTML)</h3> <p><strong>Contents:</strong> Raw HTML from web scraping</p> <p><strong>Why MongoDB?</strong></p> <ul> <li>✅ <strong>Schema flexibility</strong>: Each listing might have different fields</li> <li>✅ <strong>Nested data</strong>: HTML structure is hierarchical</li> <li>✅ <strong>Fast writes</strong>: Scraping generates data quickly</li> <li>✅ <strong>Temporal tracking</strong>: Store multiple scrapes over time</li> </ul> <p><strong>Schema:</strong></p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="dl">"</span><span class="s2">_id</span><span class="dl">"</span><span class="p">:</span> <span class="nc">ObjectId</span><span class="p">(</span><span class="dl">"</span><span class="s2">...</span><span class="dl">"</span><span class="p">),</span>
  <span class="dl">"</span><span class="s2">url</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">https://immobiliare.it/...</span><span class="dl">"</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">html</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">&lt;html&gt;...&lt;/html&gt;</span><span class="dl">"</span><span class="p">,</span>  <span class="c1">// Raw HTML</span>
  <span class="dl">"</span><span class="s2">timestamp</span><span class="dl">"</span><span class="p">:</span> <span class="nc">ISODate</span><span class="p">(</span><span class="dl">"</span><span class="s2">2025-01-15T10:30:00Z</span><span class="dl">"</span><span class="p">),</span>
  <span class="dl">"</span><span class="s2">province</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Milan</span><span class="dl">"</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">listing_type</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">rent</span><span class="dl">"</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Why not PostgreSQL here?</strong></p> <ul> <li>HTML is unstructured → No fixed schema yet</li> <li>Don’t know what fields exist until parsing</li> <li>Need flexibility during exploration</li> </ul> <h3 id="layer-2-mongodb-warehouse-structured-documents">Layer 2: MongoDB Warehouse (Structured Documents)</h3> <p><strong>Contents:</strong> Extracted, structured data (still in JSON/document format)</p> <p><strong>Why MongoDB?</strong></p> <ul> <li>✅ <strong>Still evolving schema</strong>: Discovering new fields during ETL</li> <li>✅ <strong>Nested structures</strong>: Amenities, features as sub-documents</li> <li>✅ <strong>Easy iteration</strong>: Can add/remove fields without migrations</li> </ul> <p><strong>Schema:</strong></p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="dl">"</span><span class="s2">_id</span><span class="dl">"</span><span class="p">:</span> <span class="nc">ObjectId</span><span class="p">(</span><span class="dl">"</span><span class="s2">...</span><span class="dl">"</span><span class="p">),</span>
  <span class="dl">"</span><span class="s2">listing_id</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">12345</span><span class="dl">"</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">price</span><span class="dl">"</span><span class="p">:</span> <span class="mi">1200</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">surface</span><span class="dl">"</span><span class="p">:</span> <span class="mi">85</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">rooms</span><span class="dl">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">location</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">latitude</span><span class="dl">"</span><span class="p">:</span> <span class="mf">45.4642</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">longitude</span><span class="dl">"</span><span class="p">:</span> <span class="mf">9.1900</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">province</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Milan</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">city</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Milano</span><span class="dl">"</span>
  <span class="p">},</span>
  <span class="dl">"</span><span class="s2">amenities</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">elevator</span><span class="dl">"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">balcony</span><span class="dl">"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">parking</span><span class="dl">"</span><span class="p">:</span> <span class="kc">false</span>
  <span class="p">},</span>
  <span class="dl">"</span><span class="s2">energy</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">class</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">B</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">consumption</span><span class="dl">"</span><span class="p">:</span> <span class="mf">75.5</span>
  <span class="p">},</span>
  <span class="dl">"</span><span class="s2">scraped_at</span><span class="dl">"</span><span class="p">:</span> <span class="nc">ISODate</span><span class="p">(</span><span class="dl">"</span><span class="s2">...</span><span class="dl">"</span><span class="p">),</span>
  <span class="dl">"</span><span class="s2">language</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Italian</span><span class="dl">"</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>This is the “messy middle” layer</strong> where data is structured but not yet normalized.</p> <h3 id="layer-3-postgresql-ml-ready-analytics">Layer 3: PostgreSQL (ML-Ready Analytics)</h3> <p><strong>Contents:</strong> Normalized, relational, ML-ready data</p> <p><strong>Why PostgreSQL?</strong></p> <ul> <li>✅ <strong>Relational integrity</strong>: Foreign keys, constraints</li> <li>✅ <strong>Query performance</strong>: Joins, aggregations, indexes</li> <li>✅ <strong>Data normalization</strong>: No redundancy (snowflake schema)</li> <li>✅ <strong>Type safety</strong>: Strong typing for ML features</li> <li>✅ <strong>Standard SQL</strong>: Compatible with ML tools (pandas, scikit-learn)</li> </ul> <p><strong>Schema (Snowflake):</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/italian-real-estate/PostgreSQL_warehouse_ERD-480.webp 480w,/assets/img/projects/italian-real-estate/PostgreSQL_warehouse_ERD-800.webp 800w,/assets/img/projects/italian-real-estate/PostgreSQL_warehouse_ERD-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/italian-real-estate/PostgreSQL_warehouse_ERD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PostgreSQL schema" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PostgreSQL snowflake schema with fact table (listings) and dimension tables </div> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Fact table</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">listings</span> <span class="p">(</span>
    <span class="n">listing_id</span> <span class="nb">SERIAL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span>
    <span class="n">price</span> <span class="nb">NUMERIC</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">surface</span> <span class="nb">NUMERIC</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">rooms</span> <span class="nb">INTEGER</span><span class="p">,</span>
    <span class="n">property_type_id</span> <span class="nb">INTEGER</span> <span class="k">REFERENCES</span> <span class="n">property_types</span><span class="p">(</span><span class="n">id</span><span class="p">),</span>
    <span class="n">energy_class_id</span> <span class="nb">INTEGER</span> <span class="k">REFERENCES</span> <span class="n">energy_classes</span><span class="p">(</span><span class="n">id</span><span class="p">),</span>
    <span class="n">province_id</span> <span class="nb">INTEGER</span> <span class="k">REFERENCES</span> <span class="n">provinces</span><span class="p">(</span><span class="n">id</span><span class="p">),</span>
    <span class="n">latitude</span> <span class="nb">NUMERIC</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
    <span class="n">longitude</span> <span class="nb">NUMERIC</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
    <span class="n">elevator</span> <span class="nb">BOOLEAN</span><span class="p">,</span>
    <span class="n">balcony</span> <span class="nb">BOOLEAN</span><span class="p">,</span>
    <span class="c1">-- ... 40+ more fields</span>
<span class="p">);</span>

<span class="c1">-- Dimension tables (normalized)</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">property_types</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">SERIAL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span>
    <span class="n">name</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span> <span class="k">UNIQUE</span>
<span class="p">);</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">energy_classes</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">SERIAL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span>
    <span class="k">class</span> <span class="nb">CHAR</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">UNIQUE</span><span class="p">,</span>
    <span class="n">description</span> <span class="nb">TEXT</span>
<span class="p">);</span>

<span class="c1">-- ... more dimension tables</span>
</code></pre></div></div> <p><strong>This is the final, clean layer</strong> optimized for ML model training.</p> <hr/> <h2 id="why-not-just-use-one-database">Why Not Just Use One Database?</h2> <h3 id="why-not-postgresql-for-everything">Why Not PostgreSQL for Everything?</h3> <p><strong>Could I have used PostgreSQL from the start?</strong></p> <p>Technically yes, but it would have been painful:</p> <p><strong>Problems with PostgreSQL for raw data:</strong></p> <ol> <li><strong>Schema rigidity</strong> <ul> <li>Need to define schema upfront</li> <li>Adding fields requires migrations</li> <li>Early exploration is slow</li> </ul> </li> <li><strong>Messy nested data</strong> <ul> <li>HTML parsing produces inconsistent structures</li> <li>Would need JSONB columns (loses benefits of relational DB)</li> </ul> </li> <li><strong>Rapid iteration</strong> <ul> <li>During ETL development, schema changes constantly</li> <li>ALTER TABLE for every discovery is tedious</li> </ul> </li> </ol> <p><strong>Example pain point:</strong></p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Week 1: Define schema</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">raw_listings</span> <span class="p">(</span>
    <span class="n">id</span> <span class="nb">SERIAL</span><span class="p">,</span>
    <span class="n">html</span> <span class="nb">TEXT</span><span class="p">,</span>
    <span class="n">price</span> <span class="nb">INTEGER</span>
<span class="p">);</span>

<span class="c1">-- Week 2: Realize need more fields</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">raw_listings</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">surface</span> <span class="nb">NUMERIC</span><span class="p">;</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">raw_listings</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">energy_class</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>

<span class="c1">-- Week 3: More discoveries</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">raw_listings</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">heating_type</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">);</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">raw_listings</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">building_year</span> <span class="nb">INTEGER</span><span class="p">;</span>

<span class="c1">-- Week 4: Realize structure was wrong, need to denormalize differently</span>
<span class="c1">-- ... painful restructuring</span>
</code></pre></div></div> <p>With MongoDB, I could just add fields as I discovered them:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Week 1</span>
<span class="nx">db</span><span class="p">.</span><span class="nx">listings</span><span class="p">.</span><span class="nf">insertOne</span><span class="p">({</span><span class="na">price</span><span class="p">:</span> <span class="mi">1200</span><span class="p">})</span>

<span class="c1">// Week 2</span>
<span class="nx">db</span><span class="p">.</span><span class="nx">listings</span><span class="p">.</span><span class="nf">insertOne</span><span class="p">({</span><span class="na">price</span><span class="p">:</span> <span class="mi">1200</span><span class="p">,</span> <span class="na">surface</span><span class="p">:</span> <span class="mi">85</span><span class="p">,</span> <span class="na">energy_class</span><span class="p">:</span> <span class="dl">"</span><span class="s2">B</span><span class="dl">"</span><span class="p">})</span>

<span class="c1">// Week 3</span>
<span class="nx">db</span><span class="p">.</span><span class="nx">listings</span><span class="p">.</span><span class="nf">insertOne</span><span class="p">({</span><span class="na">price</span><span class="p">:</span> <span class="mi">1200</span><span class="p">,</span> <span class="p">...,</span> <span class="na">heating_type</span><span class="p">:</span> <span class="dl">"</span><span class="s2">autonomous</span><span class="dl">"</span><span class="p">})</span>

<span class="c1">// No migrations needed!</span>
</code></pre></div></div> <h3 id="why-not-mongodb-for-everything">Why Not MongoDB for Everything?</h3> <p><strong>Could I have trained ML models directly on MongoDB?</strong></p> <p>Also yes, but suboptimal:</p> <p><strong>Problems with MongoDB for ML:</strong></p> <ol> <li><strong>Query performance</strong> <ul> <li>Joins are awkward (manual <code class="language-plaintext highlighter-rouge">$lookup</code> or client-side)</li> <li>Aggregations are verbose</li> <li>Slower than SQL for analytical queries</li> </ul> </li> <li><strong>Data integrity</strong> <ul> <li>No foreign keys → Data can be inconsistent</li> <li>No type enforcement → Strings mixed with numbers</li> <li>ML models need clean, consistent data</li> </ul> </li> <li><strong>ML tooling</strong> <ul> <li>Pandas, scikit-learn expect flat, relational data</li> <li>Would need to flatten JSON every time</li> </ul> </li> </ol> <p><strong>Example query comparison:</strong></p> <p><strong>MongoDB</strong> (get average price by property type):</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">db</span><span class="p">.</span><span class="nx">listings</span><span class="p">.</span><span class="nf">aggregate</span><span class="p">([</span>
  <span class="p">{</span><span class="na">$group</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">_id</span><span class="p">:</span> <span class="dl">"</span><span class="s2">$property_type</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">avg_price</span><span class="p">:</span> <span class="p">{</span><span class="na">$avg</span><span class="p">:</span> <span class="dl">"</span><span class="s2">$price</span><span class="dl">"</span><span class="p">}</span>
  <span class="p">}},</span>
  <span class="p">{</span><span class="na">$sort</span><span class="p">:</span> <span class="p">{</span><span class="na">avg_price</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}}</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>PostgreSQL</strong> (same query):</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">pt</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="k">AVG</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">price</span><span class="p">)</span> <span class="k">as</span> <span class="n">avg_price</span>
<span class="k">FROM</span> <span class="n">listings</span> <span class="n">l</span>
<span class="k">JOIN</span> <span class="n">property_types</span> <span class="n">pt</span> <span class="k">ON</span> <span class="n">l</span><span class="p">.</span><span class="n">property_type_id</span> <span class="o">=</span> <span class="n">pt</span><span class="p">.</span><span class="n">id</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">pt</span><span class="p">.</span><span class="n">name</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">avg_price</span> <span class="k">DESC</span><span class="p">;</span>
</code></pre></div></div> <p>For simple queries, MongoDB is fine. But ML feature engineering involves <strong>complex joins, window functions, and aggregations</strong>. SQL shines here.</p> <hr/> <h2 id="the-etl-pipeline-mongodb--postgresql">The ETL Pipeline: MongoDB → PostgreSQL</h2> <h3 id="phase-1-html--structured-documents-mongodb-datalake--warehouse">Phase 1: HTML → Structured Documents (MongoDB Datalake → Warehouse)</h3> <p><strong>Airflow DAG:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="n">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>

<span class="k">with</span> <span class="nc">DAG</span><span class="p">(</span><span class="sh">'</span><span class="s">mongodb_etl</span><span class="sh">'</span><span class="p">,</span> <span class="n">schedule</span><span class="o">=</span><span class="sh">'</span><span class="s">@daily</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">extract_from_html</span><span class="p">():</span>
        <span class="sh">"""</span><span class="s">Parse HTML and extract structured fields</span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
        <span class="kn">import</span> <span class="n">pymongo</span>

        <span class="n">mongo</span> <span class="o">=</span> <span class="n">pymongo</span><span class="p">.</span><span class="nc">MongoClient</span><span class="p">()</span>
        <span class="n">datalake</span> <span class="o">=</span> <span class="n">mongo</span><span class="p">[</span><span class="sh">'</span><span class="s">real_estate_datalake</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">warehouse</span> <span class="o">=</span> <span class="n">mongo</span><span class="p">[</span><span class="sh">'</span><span class="s">real_estate_warehouse</span><span class="sh">'</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">datalake</span><span class="p">[</span><span class="sh">'</span><span class="s">raw_listings</span><span class="sh">'</span><span class="p">].</span><span class="nf">find</span><span class="p">():</span>
            <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">html</span><span class="sh">'</span><span class="p">],</span> <span class="sh">'</span><span class="s">html.parser</span><span class="sh">'</span><span class="p">)</span>

            <span class="c1"># Extract fields
</span>            <span class="n">listing</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">'</span><span class="s">listing_id</span><span class="sh">'</span><span class="p">:</span> <span class="nf">extract_listing_id</span><span class="p">(</span><span class="n">soup</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">:</span> <span class="nf">extract_price</span><span class="p">(</span><span class="n">soup</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">surface</span><span class="sh">'</span><span class="p">:</span> <span class="nf">extract_surface</span><span class="p">(</span><span class="n">soup</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">:</span> <span class="nf">extract_location</span><span class="p">(</span><span class="n">soup</span><span class="p">),</span>
                <span class="c1"># ... 50+ more fields
</span>            <span class="p">}</span>

            <span class="n">warehouse</span><span class="p">[</span><span class="sh">'</span><span class="s">listings</span><span class="sh">'</span><span class="p">].</span><span class="nf">insert_one</span><span class="p">(</span><span class="n">listing</span><span class="p">)</span>

    <span class="n">extract_task</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="sh">'</span><span class="s">extract_structured_data</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">extract_from_html</span>
    <span class="p">)</span>
</code></pre></div></div> <p><strong>Key steps:</strong></p> <ol> <li>Read raw HTML from datalake</li> <li>Parse with BeautifulSoup</li> <li>Extract structured fields</li> <li>Store in warehouse (still MongoDB)</li> </ol> <h3 id="phase-2-structured-documents--relational-mongodb--postgresql">Phase 2: Structured Documents → Relational (MongoDB → PostgreSQL)</h3> <p><strong>Migration script:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pymongo</span>
<span class="kn">import</span> <span class="n">psycopg2</span>
<span class="kn">from</span> <span class="n">psycopg2.extras</span> <span class="kn">import</span> <span class="n">execute_batch</span>

<span class="k">def</span> <span class="nf">migrate_to_postgresql</span><span class="p">():</span>
    <span class="c1"># Connect to both databases
</span>    <span class="n">mongo</span> <span class="o">=</span> <span class="n">pymongo</span><span class="p">.</span><span class="nc">MongoClient</span><span class="p">()</span>
    <span class="n">pg_conn</span> <span class="o">=</span> <span class="n">psycopg2</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="sh">"</span><span class="s">dbname=real_estate ...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pg_cursor</span> <span class="o">=</span> <span class="n">pg_conn</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>

    <span class="c1"># Migrate dimension tables first (for foreign keys)
</span>    <span class="n">property_types</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="n">energy_classes</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mongo</span><span class="p">[</span><span class="sh">'</span><span class="s">real_estate_warehouse</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">listings</span><span class="sh">'</span><span class="p">].</span><span class="nf">find</span><span class="p">():</span>
        <span class="n">property_types</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">property_type</span><span class="sh">'</span><span class="p">))</span>
        <span class="n">energy_classes</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">energy_class</span><span class="sh">'</span><span class="p">))</span>

    <span class="c1"># Insert dimension tables
</span>    <span class="nf">execute_batch</span><span class="p">(</span><span class="n">pg_cursor</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">INSERT INTO property_types (name) VALUES (%s) ON CONFLICT DO NOTHING</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">[(</span><span class="n">pt</span><span class="p">,)</span> <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">property_types</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># ... same for energy_classes, provinces, etc.
</span>
    <span class="c1"># Migrate fact table (listings)
</span>    <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mongo</span><span class="p">[</span><span class="sh">'</span><span class="s">real_estate_warehouse</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">listings</span><span class="sh">'</span><span class="p">].</span><span class="nf">find</span><span class="p">():</span>
        <span class="n">row</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">surface</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">rooms</span><span class="sh">'</span><span class="p">],</span>
            <span class="nf">lookup_property_type_id</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">property_type</span><span class="sh">'</span><span class="p">]),</span>
            <span class="nf">lookup_energy_class_id</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">energy_class</span><span class="sh">'</span><span class="p">]),</span>
            <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="p">],</span>
            <span class="c1"># ... 40+ more fields
</span>        <span class="p">)</span>
        <span class="n">batch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1000</span><span class="p">:</span>
            <span class="nf">execute_batch</span><span class="p">(</span><span class="n">pg_cursor</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">INSERT INTO listings (price, surface, ...) VALUES (%s, %s, ...)</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">batch</span>
            <span class="p">)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">pg_conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Key steps:</strong></p> <ol> <li>Extract unique values for dimension tables</li> <li>Insert dimension tables first (for FK references)</li> <li>Flatten nested JSON into relational rows</li> <li>Batch insert for performance</li> </ol> <hr/> <h2 id="the-translation-layer-italian--english">The Translation Layer: Italian → English</h2> <p>One complication: All scraped data was in Italian, but I wanted ML models to work with English.</p> <p><strong>Where to translate?</strong> Between MongoDB warehouse and PostgreSQL.</p> <h3 id="translation-pipeline">Translation Pipeline</h3> <p><strong>Architecture:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MongoDB Warehouse (Italian)
         ↓
   Translation Layer
    - LibreTranslate API
    - SQLite cache
    - Custom dictionary
         ↓
PostgreSQL (English)
</code></pre></div></div> <p><strong>Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">sqlite3</span>
<span class="kn">import</span> <span class="n">requests</span>

<span class="k">class</span> <span class="nc">TranslationCache</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="sh">'</span><span class="s">translation_cache.db</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">create_table</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">create_table</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">'''</span><span class="s">
            CREATE TABLE IF NOT EXISTS translations (
                italian TEXT PRIMARY KEY,
                english TEXT,
                field_name TEXT
            )
        </span><span class="sh">'''</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">field</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Translate with caching</span><span class="sh">"""</span>
        <span class="c1"># Check cache
</span>        <span class="n">cached</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span>
            <span class="sh">'</span><span class="s">SELECT english FROM translations WHERE italian = ?</span><span class="sh">'</span><span class="p">,</span>
            <span class="p">(</span><span class="n">text</span><span class="p">,)</span>
        <span class="p">).</span><span class="nf">fetchone</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">cached</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cached</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Check custom dictionary
</span>        <span class="k">if</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">REAL_ESTATE_DICTIONARY</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">REAL_ESTATE_DICTIONARY</span><span class="p">[</span><span class="n">text</span><span class="p">]</span>

        <span class="c1"># Call LibreTranslate API
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">'</span><span class="s">http://localhost:5000/translate</span><span class="sh">'</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span>
            <span class="sh">'</span><span class="s">q</span><span class="sh">'</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">it</span><span class="sh">'</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">en</span><span class="sh">'</span>
        <span class="p">})</span>
        <span class="n">translated</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()[</span><span class="sh">'</span><span class="s">translatedText</span><span class="sh">'</span><span class="p">]</span>

        <span class="c1"># Cache result
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span>
            <span class="sh">'</span><span class="s">INSERT INTO translations VALUES (?, ?, ?)</span><span class="sh">'</span><span class="p">,</span>
            <span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">translated</span><span class="p">,</span> <span class="n">field</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">translated</span>

<span class="c1"># Custom real estate dictionary
</span><span class="n">REAL_ESTATE_DICTIONARY</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">cucina abitabile</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">eat-in kitchen</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">occupato</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">inhabited</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">libero</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">vacant</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">piano terra</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ground floor</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">ultimo piano</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">top floor</span><span class="sh">'</span><span class="p">,</span>
    <span class="c1"># ... 100+ phrases
</span><span class="p">}</span>
</code></pre></div></div> <p><strong>Performance:</strong></p> <ul> <li><strong>1M+ translations needed</strong></li> <li><strong>Cache hit rate</strong>: ~80% (many repeated phrases)</li> <li><strong>Custom dictionary</strong>: Handles domain-specific jargon</li> <li><strong>Total time</strong>: ~6 hours (would be 30+ hours without caching)</li> </ul> <hr/> <h2 id="performance-comparison-mongodb-vs-postgresql">Performance Comparison: MongoDB vs PostgreSQL</h2> <p>I ran the same analytical queries on both databases to compare:</p> <table> <thead> <tr> <th><strong>Query</strong></th> <th><strong>MongoDB</strong></th> <th><strong>PostgreSQL</strong></th> <th><strong>Winner</strong></th> </tr> </thead> <tbody> <tr> <td>Simple filter (price &gt; 1000)</td> <td>120ms</td> <td>45ms</td> <td>PostgreSQL</td> </tr> <tr> <td>Group by + aggregate</td> <td>850ms</td> <td>180ms</td> <td>PostgreSQL</td> </tr> <tr> <td>Join 3 tables</td> <td>2.1s (manual)</td> <td>320ms</td> <td>PostgreSQL</td> </tr> <tr> <td>Complex analytics query</td> <td>5.8s</td> <td>1.2s</td> <td>PostgreSQL</td> </tr> <tr> <td>Full table scan</td> <td>3.2s</td> <td>2.8s</td> <td>PostgreSQL</td> </tr> </tbody> </table> <p><strong>For ML feature engineering</strong> (lots of joins, aggregations, window functions): <strong>PostgreSQL is 3-5× faster</strong>.</p> <hr/> <h2 id="lessons-learned">Lessons Learned</h2> <h3 id="1-use-the-right-tool-for-each-stage">1. Use the Right Tool for Each Stage</h3> <p><strong>MongoDB for early, exploratory stages:</strong></p> <ul> <li>Schema is evolving</li> <li>Data is messy/nested</li> <li>Fast iteration matters more than query performance</li> </ul> <p><strong>PostgreSQL for analytics and ML:</strong></p> <ul> <li>Schema is stable</li> <li>Need complex queries</li> <li>Data integrity matters</li> </ul> <p><strong>Don’t force one database to do everything.</strong></p> <h3 id="2-data-transformation-is-iterative">2. Data Transformation Is Iterative</h3> <p>My ETL pipeline evolved over weeks:</p> <ul> <li>Started with 20 fields → Ended with 50+</li> <li>Discovered nested structures that needed flattening</li> <li>Realized some fields needed custom parsing</li> </ul> <p><strong>MongoDB’s flexibility let me iterate fast</strong> without constant schema migrations.</p> <h3 id="3-translation-at-the-right-layer">3. Translation at the Right Layer</h3> <p>I initially considered three options:</p> <ol> <li>Translate during scraping (too early, brittle)</li> <li>Translate in MongoDB (wrong layer, should stay raw)</li> <li><strong>Translate during MongoDB → PostgreSQL migration</strong> ← This worked</li> </ol> <p><strong>Why option 3?</strong></p> <ul> <li>MongoDB has raw Italian (preserves source)</li> <li>Translation happens once in batch (efficient)</li> <li>PostgreSQL gets clean English (ready for ML)</li> </ul> <h3 id="4-normalization-reduces-redundancy">4. Normalization Reduces Redundancy</h3> <p>In MongoDB warehouse, “Apartment” appeared 400,000+ times as a string.</p> <p>In PostgreSQL:</p> <ul> <li><code class="language-plaintext highlighter-rouge">property_types</code> table has one row: <code class="language-plaintext highlighter-rouge">(1, 'Apartment')</code></li> <li><code class="language-plaintext highlighter-rouge">listings</code> table references: <code class="language-plaintext highlighter-rouge">property_type_id = 1</code></li> </ul> <p><strong>Storage savings:</strong> ~40% reduction in database size</p> <p><strong>Query benefits:</strong> Categorical features are indexed integers, not strings</p> <h3 id="5-start-simple-add-complexity-when-needed">5. Start Simple, Add Complexity When Needed</h3> <p><strong>Phase 1:</strong> I just used MongoDB (weeks 1-3)</p> <ul> <li>Got pipeline working</li> <li>Explored data structure</li> <li>Proved the concept</li> </ul> <p><strong>Phase 2:</strong> Added PostgreSQL (weeks 4-6)</p> <ul> <li>Optimized for ML</li> <li>Improved query performance</li> <li>Enforced data quality</li> </ul> <p><strong>I didn’t architect the “perfect” system upfront.</strong> I evolved it based on needs.</p> <hr/> <h2 id="when-to-use-this-architecture">When to Use This Architecture</h2> <p><strong>Use MongoDB + PostgreSQL when:</strong></p> <p>✅ <strong>Data starts messy</strong> (web scraping, APIs, logs) ✅ <strong>Schema evolves during exploration</strong> ✅ <strong>End goal is ML/analytics</strong> (need SQL performance) ✅ <strong>You have time for ETL</strong> (not real-time streaming)</p> <p><strong>Just use PostgreSQL when:</strong></p> <p>✅ <strong>Schema is known upfront</strong> ✅ <strong>Data is already structured</strong> ✅ <strong>Mainly transactional workload</strong> (CRUD operations)</p> <p><strong>Just use MongoDB when:</strong></p> <p>✅ <strong>Schema will always be flexible</strong> ✅ <strong>Query patterns are simple</strong> (key-value lookups) ✅ <strong>Horizontal scaling is critical</strong></p> <hr/> <h2 id="code--resources">Code &amp; Resources</h2> <p>Full pipeline implementation in my <a href="https://github.com/LeonardoPaccianiMori/italian-real-estate-pipeline">Italian Real Estate repository</a>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">dags/mongodb_etl_dag.py</code> - Airflow ETL pipeline</li> <li><code class="language-plaintext highlighter-rouge">dags/postgresql_migration_dag.py</code> - MongoDB → PostgreSQL migration</li> <li><code class="language-plaintext highlighter-rouge">scripts/translate.py</code> - Translation layer</li> <li><code class="language-plaintext highlighter-rouge">sql/schema.sql</code> - PostgreSQL schema definition</li> </ul> <p><strong>Related posts:</strong></p> <ul> <li><a href="/blog/2025/synthetic-data-ctgan/">Custom Synthetic Data Algorithm</a> - Using PostgreSQL data for ML</li> <li><a href="/projects/italian-real-estate/">Italian Real Estate Project</a> - Full pipeline overview</li> </ul> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li><strong>Multi-database architectures are valid</strong> - Don’t feel obligated to use just one database</li> <li><strong>Match database to data maturity</strong> - Flexible (MongoDB) for exploration, rigid (PostgreSQL) for production</li> <li><strong>ETL is where the magic happens</strong> - Transform data to fit the right tool at each stage</li> <li><strong>Normalization matters for ML</strong> - Clean, typed, non-redundant data = better models</li> <li><strong>Start simple, evolve based on needs</strong> - Don’t over-architect upfront</li> </ol> <p>What database architecture have you used for ML projects? Have you found multi-database pipelines useful? Share your experiences!</p>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="databases"/><category term="architecture"/><summary type="html"><![CDATA[Why I used both NoSQL and SQL databases in my data pipeline, and when each makes sense]]></summary></entry><entry><title type="html">Why I Built a Custom Synthetic Data Algorithm Instead of Using CTGAN</title><link href="https://unique-cocada-065d88.netlify.app//blog/2025/synthetic-data-ctgan/" rel="alternate" type="text/html" title="Why I Built a Custom Synthetic Data Algorithm Instead of Using CTGAN"/><published>2025-05-15T09:00:00+00:00</published><updated>2025-05-15T09:00:00+00:00</updated><id>https://unique-cocada-065d88.netlify.app//blog/2025/synthetic-data-ctgan</id><content type="html" xml:base="https://unique-cocada-065d88.netlify.app//blog/2025/synthetic-data-ctgan/"><![CDATA[<h2 id="the-problem-1-million-synthetic-real-estate-listings">The Problem: 1 Million Synthetic Real Estate Listings</h2> <p>For my <a href="/projects/italian-real-estate/">Italian Real Estate Analysis project</a>, I needed to generate synthetic data. A lot of it.</p> <p><strong>The requirements:</strong></p> <ul> <li>Create ~1 million synthetic property listings</li> <li>Preserve statistical distributions from real data</li> <li><strong>Maintain critical feature correlations</strong> (especially location ↔ price)</li> <li>Ensure geographic coherence (coordinates within Italy’s borders)</li> <li>Support multiple property types, energy classes, and features</li> </ul> <p><strong>Why synthetic data?</strong> Two reasons:</p> <ol> <li><strong>Privacy</strong>: Don’t publish exact real property data scraped from websites</li> <li><strong>Augmentation</strong>: Increase dataset size for better ML training</li> </ol> <p>The obvious solution? Use <a href="https://github.com/sdv-dev/CTGAN">CTGAN</a>, a state-of-the-art generative AI model designed specifically for tabular data. It’s based on GANs (Generative Adversarial Networks) and has been shown to work well for generating realistic synthetic datasets.</p> <p><strong>Spoiler:</strong> CTGAN failed spectacularly for my use case. Here’s why, and how I built something better.</p> <hr/> <h2 id="attempt-1-ctgan-the-obvious-solution">Attempt #1: CTGAN (The “Obvious” Solution)</h2> <p>CTGAN seemed perfect on paper:</p> <p>✅ <strong>Purpose-built for tabular data</strong> (unlike image-focused GANs) ✅ <strong>Handles mixed data types</strong> (categorical + continuous features) ✅ <strong>Preserves distributions</strong> well in benchmarks ✅ <strong>Actively maintained</strong> by the Synthetic Data Vault team ✅ <strong>Easy to use</strong> with a simple Python API</p> <h3 id="the-implementation">The Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">ctgan</span> <span class="kn">import</span> <span class="n">CTGAN</span>

<span class="c1"># Load real real estate data
</span><span class="n">real_data</span> <span class="o">=</span> <span class="nf">load_rental_listings</span><span class="p">()</span>  <span class="c1"># ~80,000 listings
</span>
<span class="c1"># Train CTGAN
</span><span class="n">ctgan</span> <span class="o">=</span> <span class="nc">CTGAN</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">ctgan</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">real_data</span><span class="p">,</span> <span class="n">discrete_columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">property_type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">energy_class</span><span class="sh">'</span><span class="p">,</span> <span class="p">...])</span>

<span class="c1"># Generate synthetic data
</span><span class="n">synthetic_data</span> <span class="o">=</span> <span class="n">ctgan</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">)</span>
</code></pre></div></div> <p>Simple, right?</p> <h3 id="the-results-beautiful-distributions-broken-correlations">The Results: Beautiful Distributions, Broken Correlations</h3> <p>After training for several hours, CTGAN produced synthetic data with:</p> <p>✅ <strong>Price distribution</strong>: Nearly identical to real data ✅ <strong>Surface area distribution</strong>: Spot on ✅ <strong>Geographic distribution</strong>: Covered Italy properly ✅ <strong>Categorical features</strong>: Realistic proportions</p> <p><strong>But there was a critical problem:</strong></p> <p>❌ <strong>Location ↔ Price correlation was destroyed</strong></p> <p>In the real data:</p> <ul> <li>Properties in Milan (expensive city) → High prices</li> <li>Properties in rural areas → Low prices</li> <li>Coastal properties → Premium pricing</li> <li>Northern cities → Generally more expensive than southern</li> </ul> <p>In CTGAN’s synthetic data:</p> <ul> <li>Milan apartments priced like rural farmhouses</li> <li>Tiny studios in Rome costing more than villas in Tuscany</li> <li><strong>Complete randomness between location and price</strong></li> </ul> <h3 id="why-this-matters">Why This Matters</h3> <p>For my ML model (predicting rental prices), location is <strong>the most important feature</strong>. If synthetic data breaks the location-price relationship, it’s worthless for training.</p> <p><strong>Validation check I ran:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Real data: R² between (lat, lon) and price
</span><span class="n">real_r2</span> <span class="o">=</span> <span class="nf">measure_location_price_correlation</span><span class="p">(</span><span class="n">real_data</span><span class="p">)</span>  <span class="c1"># 0.62
</span>
<span class="c1"># CTGAN synthetic data
</span><span class="n">ctgan_r2</span> <span class="o">=</span> <span class="nf">measure_location_price_correlation</span><span class="p">(</span><span class="n">synthetic_data</span><span class="p">)</span>  <span class="c1"># 0.03
</span></code></pre></div></div> <p>The geographic-price relationship was <strong>obliterated</strong>.</p> <hr/> <h2 id="why-ctgan-failed-the-architecture-limitation">Why CTGAN Failed: The Architecture Limitation</h2> <p>CTGAN is brilliant at learning <strong>marginal distributions</strong> (how individual features are distributed), but struggles with <strong>complex multivariate relationships</strong>.</p> <h3 id="how-ctgan-works-simplified">How CTGAN Works (Simplified)</h3> <ol> <li><strong>Generator</strong>: Creates fake rows</li> <li><strong>Discriminator</strong>: Tries to spot fakes</li> <li><strong>Training</strong>: Generator gets better at fooling discriminator</li> <li><strong>Conditional generation</strong>: Uses class labels to guide generation</li> </ol> <p><strong>The problem:</strong> With 50+ features and complex geographic relationships, CTGAN couldn’t maintain the intricate correlations between:</p> <ul> <li>Latitude ↔ Price</li> <li>Longitude ↔ Price</li> <li>(Lat, Lon) ↔ Property Type</li> <li>(Lat, Lon) ↔ Energy Class</li> <li>And dozens of other location-dependent patterns</li> </ul> <p>It learned “Milan exists” and “expensive properties exist,” but not “Milan properties ARE expensive.”</p> <hr/> <h2 id="the-solution-custom-k-nearest-neighbors-algorithm">The Solution: Custom K-Nearest Neighbors Algorithm</h2> <p>I needed an approach that <strong>preserves correlations by design</strong>, not by hoping a neural network learns them.</p> <h3 id="the-core-insight">The Core Insight</h3> <p>Instead of training a model to learn correlations, <strong>find similar real examples and blend them</strong>.</p> <p><strong>Algorithm concept:</strong></p> <ol> <li>For each synthetic listing to generate: <ul> <li>Find K=5 <strong>similar real listings</strong> (based on location, price, size)</li> <li>Weight them by similarity (closer = higher weight)</li> <li>Create synthetic listing by <strong>blending</strong> their features</li> </ul> </li> <li>Repeat 1 million times</li> </ol> <p><strong>Why this works:</strong> If you blend 5 real Milan apartments, you get a realistic synthetic Milan apartment. The location-price correlation is <strong>preserved by construction</strong>.</p> <hr/> <h2 id="implementation-details">Implementation Details</h2> <h3 id="distance-metric">Distance Metric</h3> <p>I used a weighted Euclidean distance across 4 key features:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_distance</span><span class="p">(</span><span class="n">synthetic_point</span><span class="p">,</span> <span class="n">real_data</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculate weighted distance between synthetic point and all real listings
    </span><span class="sh">"""</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">surface</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.25</span>
    <span class="p">}</span>

    <span class="c1"># Normalize features to [0, 1] scale
</span>    <span class="n">normalized_real</span> <span class="o">=</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">real_data</span><span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="nf">keys</span><span class="p">())])</span>
    <span class="n">normalized_synthetic</span> <span class="o">=</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">synthetic_point</span><span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="nf">keys</span><span class="p">())])</span>

    <span class="c1"># Weighted Euclidean distance
</span>    <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span>
        <span class="p">((</span><span class="n">normalized_real</span> <span class="o">-</span> <span class="n">normalized_synthetic</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="nf">list</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="nf">values</span><span class="p">())).</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">distances</span>
</code></pre></div></div> <p><strong>Why these 4 features?</strong></p> <ul> <li><strong>Price + Surface</strong>: Define the property’s market segment</li> <li><strong>Lat + Lon</strong>: Define geographic location and context</li> <li>Together, they capture the most critical correlations</li> </ul> <h3 id="feature-blending-strategy">Feature Blending Strategy</h3> <p>For each feature type, different blending approach:</p> <p><strong>Numerical features</strong> (price, surface, rooms, etc.):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Weighted average using inverse distance weights
</span><span class="n">weights</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">distances</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># Avoid division by zero
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>    <span class="c1"># Normalize to sum to 1
</span>
<span class="n">synthetic_price</span> <span class="o">=</span> <span class="p">(</span><span class="n">real_prices</span> <span class="o">*</span> <span class="n">weights</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Categorical features</strong> (property_type, energy_class, etc.):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Weighted voting: most common value among neighbors
</span><span class="n">votes</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">neighbor_idx</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">neighbor_indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">category</span> <span class="o">=</span> <span class="n">real_data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">neighbor_idx</span><span class="p">][</span><span class="sh">'</span><span class="s">property_type</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">votes</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="o">=</span> <span class="n">votes</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">weight</span>

<span class="n">synthetic_property_type</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">votes</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">votes</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Boolean features</strong> (elevator, balcony, terrace, etc.):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Probabilistic: weighted average treated as probability
</span><span class="n">prob_has_elevator</span> <span class="o">=</span> <span class="p">(</span><span class="n">neighbor_elevator_values</span> <span class="o">*</span> <span class="n">weights</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">synthetic_has_elevator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prob_has_elevator</span><span class="p">)</span>
</code></pre></div></div> <h3 id="gpu-acceleration">GPU Acceleration</h3> <p>Computing distances for 1M synthetic points against 80K real points is <strong>expensive</strong>: 80 billion distance calculations.</p> <p><strong>CPU implementation</strong>: ~6 hours <strong>GPU implementation (TensorFlow)</strong>: ~36 minutes</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">def</span> <span class="nf">compute_distances_gpu</span><span class="p">(</span><span class="n">synthetic_batch</span><span class="p">,</span> <span class="n">real_data_tensor</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Vectorized distance computation on GPU
    </span><span class="sh">"""</span>
    <span class="c1"># Expand dimensions for broadcasting
</span>    <span class="n">synthetic_expanded</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">synthetic_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [batch, 1, features]
</span>    <span class="n">real_expanded</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">real_data_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>      <span class="c1"># [1, n_real, features]
</span>
    <span class="c1"># Compute all pairwise distances at once
</span>    <span class="n">diff</span> <span class="o">=</span> <span class="n">synthetic_expanded</span> <span class="o">-</span> <span class="n">real_expanded</span>                <span class="c1"># [batch, n_real, features]
</span>    <span class="n">weighted_diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">weights</span>                      <span class="c1"># Apply feature weights
</span>    <span class="n">distances</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">weighted_diff</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># [batch, n_real]
</span>
    <span class="k">return</span> <span class="n">distances</span>
</code></pre></div></div> <p><strong>Batch processing:</strong> Generate 1000 synthetic listings at a time to maximize GPU utilization without running out of memory.</p> <hr/> <h2 id="results-validation">Results: Validation</h2> <h3 id="correlation-preservation">Correlation Preservation</h3> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Real Data</strong></th> <th><strong>CTGAN</strong></th> <th><strong>Custom KNN</strong></th> </tr> </thead> <tbody> <tr> <td>Location ↔ Price R²</td> <td>0.62</td> <td>0.03</td> <td>0.58</td> </tr> <tr> <td>Surface ↔ Price R²</td> <td>0.71</td> <td>0.69</td> <td>0.70</td> </tr> <tr> <td>Milan price premium</td> <td>+45%</td> <td>+2%</td> <td>+42%</td> </tr> </tbody> </table> <p><strong>Result:</strong> Custom KNN preserves critical correlations while CTGAN destroys them.</p> <h3 id="distribution-quality">Distribution Quality</h3> <p>Both approaches produced good marginal distributions:</p> <ul> <li><strong>Price</strong>: Realistic right-skewed distribution</li> <li><strong>Surface area</strong>: Matches real data</li> <li><strong>Geographic spread</strong>: Covers all Italian provinces</li> <li><strong>Property type proportions</strong>: Realistic mix</li> </ul> <h3 id="geographic-coherence">Geographic Coherence</h3> <p><strong>CTGAN occasionally generated:</strong></p> <ul> <li>Properties in the Mediterranean Sea</li> <li>Coordinates outside Italy’s borders</li> <li>Impossible lat/lon combinations</li> </ul> <p><strong>Custom KNN:</strong></p> <ul> <li>✅ All coordinates within Italy (guaranteed by design)</li> <li>✅ Properties cluster in cities (like real data)</li> <li>✅ Rural properties in rural areas, urban in urban areas</li> </ul> <hr/> <h2 id="performance-comparison">Performance Comparison</h2> <table> <thead> <tr> <th><strong>Aspect</strong></th> <th><strong>CTGAN</strong></th> <th><strong>Custom KNN</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Training time</strong></td> <td>4-6 hours (GPU)</td> <td>N/A (no training)</td> </tr> <tr> <td><strong>Generation time</strong></td> <td>~2 minutes (1M samples)</td> <td>~36 minutes (1M samples, GPU)</td> </tr> <tr> <td><strong>Memory usage</strong></td> <td>Moderate</td> <td>High (distance matrices)</td> </tr> <tr> <td><strong>Correlation preservation</strong></td> <td>❌ Poor</td> <td>✅ Excellent</td> </tr> <tr> <td><strong>Scalability</strong></td> <td>Good (once trained)</td> <td>Moderate (needs real data access)</td> </tr> <tr> <td><strong>Code complexity</strong></td> <td>Low (library)</td> <td>Medium (custom implementation)</td> </tr> </tbody> </table> <hr/> <h2 id="code-example-full-pipeline">Code Example: Full Pipeline</h2> <details> <summary><strong>Click to expand: Complete implementation</strong></summary> ```python import numpy as np import pandas as pd import tensorflow as tf from sklearn.preprocessing import MinMaxScaler class SyntheticDataGenerator: def __init__(self, real_data, k_neighbors=5): self.real_data = real_data self.k = k_neighbors self.scaler = MinMaxScaler() # Prepare normalized real data for distance computation self.feature_cols = ['price', 'surface', 'latitude', 'longitude'] self.normalized_real = self.scaler.fit_transform( real_data[self.feature_cols] ) def generate(self, n_synthetic, batch_size=1000): """Generate n synthetic listings""" synthetic_data = [] for batch_start in range(0, n_synthetic, batch_size): batch_size_actual = min(batch_size, n_synthetic - batch_start) # Sample anchor points from real data anchor_indices = np.random.choice( len(self.real_data), size=batch_size_actual, replace=True ) anchors = self.normalized_real[anchor_indices] # Add small random noise to anchors noise = np.random.normal(0, 0.05, anchors.shape) synthetic_points = np.clip(anchors + noise, 0, 1) # Find K nearest neighbors for each synthetic point distances = self.compute_distances_gpu( tf.constant(synthetic_points, dtype=tf.float32) ) # Get K nearest neighbor indices _, neighbor_indices = tf.nn.top_k( -distances, # Negative because top_k returns largest k=self.k ) neighbor_indices = neighbor_indices.numpy() # Blend features from neighbors batch_synthetic = self.blend_features( synthetic_points, neighbor_indices, distances.numpy() ) synthetic_data.append(batch_synthetic) if (batch_start // batch_size + 1) % 10 == 0: print(f"Generated {batch_start + batch_size_actual}/{n_synthetic} listings") return pd.concat(synthetic_data, ignore_index=True) def compute_distances_gpu(self, synthetic_batch): """Vectorized GPU distance computation""" # Convert to TensorFlow tensors real_tensor = tf.constant(self.normalized_real, dtype=tf.float32) # Broadcasting for pairwise distances synthetic_expanded = tf.expand_dims(synthetic_batch, 1) real_expanded = tf.expand_dims(real_tensor, 0) # Weighted Euclidean distance diff_squared = (synthetic_expanded - real_expanded) ** 2 distances = tf.sqrt(tf.reduce_sum(diff_squared, axis=2)) return distances def blend_features(self, synthetic_points, neighbor_indices, distances): """Blend features from neighbors""" synthetic_rows = [] for i, neighbors in enumerate(neighbor_indices): # Get neighbor data neighbor_data = self.real_data.iloc[neighbors] # Compute weights (inverse distance) neighbor_distances = distances[i, neighbors] weights = 1 / (neighbor_distances + 1e-6) weights = weights / weights.sum() # Blend features synthetic_row = {} # Numerical features: weighted average for col in ['price', 'surface', 'rooms', 'bathrooms', ...]: if col in neighbor_data.columns: synthetic_row[col] = (neighbor_data[col] * weights).sum() # Categorical features: weighted voting for col in ['property_type', 'energy_class', 'heating_type', ...]: if col in neighbor_data.columns: votes = {} for j, weight in enumerate(weights): value = neighbor_data.iloc[j][col] votes[value] = votes.get(value, 0) + weight synthetic_row[col] = max(votes, key=votes.get) # Boolean features: probabilistic for col in ['elevator', 'balcony', 'terrace', ...]: if col in neighbor_data.columns: prob = (neighbor_data[col].astype(float) * weights).sum() synthetic_row[col] = np.random.binomial(1, prob) synthetic_rows.append(synthetic_row) return pd.DataFrame(synthetic_rows) # Usage generator = SyntheticDataGenerator(real_rental_data, k_neighbors=5) synthetic_data = generator.generate(n_synthetic=1_000_000, batch_size=1000) ``` </details> <hr/> <h2 id="lessons-learned">Lessons Learned</h2> <h3 id="1-off-the-shelf-isnt-always-best">1. Off-the-Shelf Isn’t Always Best</h3> <p>CTGAN is a sophisticated, well-engineered library. But for my specific use case (geographic data with strong location correlations), it wasn’t the right tool.</p> <p><strong>When to use CTGAN:</strong></p> <ul> <li>Features are mostly independent</li> <li>You need perfect marginal distributions</li> <li>You have lots of training data</li> <li>Correlations are simple/linear</li> </ul> <p><strong>When to build custom:</strong></p> <ul> <li>Domain-specific correlations are critical</li> <li>You understand the feature relationships</li> <li>You need guaranteed properties (e.g., geographic coherence)</li> <li>You have clear validation criteria</li> </ul> <h3 id="2-sometimes-simpler-is-better">2. Sometimes Simpler Is Better</h3> <p>My KNN approach is conceptually simpler than CTGAN (no neural networks, no adversarial training), yet it worked better for this problem.</p> <p><strong>Why?</strong> Because it exploits <strong>domain knowledge</strong>: “Similar properties should be geographically close.” CTGAN had to learn this from scratch, and failed.</p> <h3 id="3-validation-is-everything">3. Validation Is Everything</h3> <p>Without rigorous validation (correlation checks, geographic coherence, distribution comparisons), I might have shipped CTGAN-generated data and wondered why my ML model performed poorly.</p> <p><strong>Validation saved me from:</strong></p> <ul> <li>Training a price prediction model on nonsense data</li> <li>Wasting weeks debugging model architecture</li> <li>Producing a broken final product</li> </ul> <h3 id="4-gpu-acceleration-makes-custom-solutions-viable">4. GPU Acceleration Makes Custom Solutions Viable</h3> <p>Without GPU acceleration, my KNN approach would take ~6 hours (vs. CTGAN’s 2 minutes for generation). That’s a dealbreaker.</p> <p>With GPU: ~36 minutes. Totally acceptable for a one-time data generation task.</p> <p><strong>Takeaway:</strong> Don’t dismiss custom algorithms because they seem slow. Profile first, then optimize with GPU/vectorization.</p> <hr/> <h2 id="when-would-i-use-ctgan">When Would I Use CTGAN?</h2> <p>Despite failing here, CTGAN is still a powerful tool. I’d use it for:</p> <ol> <li><strong>Financial/transaction data</strong> (less geographic dependency)</li> <li><strong>Medical records</strong> (need differential privacy guarantees, which CTGAN supports)</li> <li><strong>Customer demographics</strong> (simpler correlations)</li> <li><strong>Exploratory analysis</strong> (quick synthetic data for testing pipelines)</li> </ol> <p>But for <strong>geospatial data</strong> or datasets with <strong>known, strong correlations</strong>, custom approaches can work better.</p> <hr/> <h2 id="code--resources">Code &amp; Resources</h2> <p>Full implementation available in my <a href="https://github.com/LeonardoPaccianiMori/italian-real-estate-pipeline">Italian Real Estate Pipeline repository</a> (see <code class="language-plaintext highlighter-rouge">notebooks/synthetic_data_generation.ipynb</code>).</p> <p><strong>Related reading:</strong></p> <ul> <li><a href="https://arxiv.org/abs/1907.00503">CTGAN Paper</a> - Original research</li> <li><a href="https://sdv.dev/SDV/">SDV Documentation</a> - Synthetic Data Vault library</li> <li><a href="/projects/italian-real-estate/">My Real Estate Project</a> - Full pipeline context</li> </ul> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li><strong>Validate synthetic data rigorously</strong> - Check correlations, not just distributions</li> <li><strong>Domain knowledge &gt; Generic algorithms</strong> - When you know critical relationships, encode them directly</li> <li><strong>GPU acceleration unlocks custom solutions</strong> - 10× speedups make “slow” algorithms viable</li> <li><strong>Off-the-shelf tools are great starting points</strong> - But don’t be afraid to build custom when needed</li> <li><strong>Geographic data needs special care</strong> - Spatial correlations are complex and critical</li> </ol> <p>Have you worked with synthetic data generation? What approaches have worked (or failed) for you? I’d love to hear about your experiences!</p>]]></content><author><name></name></author><category term="data-science"/><category term="machine-learning"/><category term="synthetic-data"/><category term="algorithms"/><summary type="html"><![CDATA[When off-the-shelf ML isn't enough—designing a KNN-based synthetic data generator that preserves feature correlations]]></summary></entry><entry><title type="html">When Simpler Models Win: Comparing 7 CNN Architectures</title><link href="https://unique-cocada-065d88.netlify.app//blog/2025/simple-cnn-wins/" rel="alternate" type="text/html" title="When Simpler Models Win: Comparing 7 CNN Architectures"/><published>2025-02-05T11:00:00+00:00</published><updated>2025-02-05T11:00:00+00:00</updated><id>https://unique-cocada-065d88.netlify.app//blog/2025/simple-cnn-wins</id><content type="html" xml:base="https://unique-cocada-065d88.netlify.app//blog/2025/simple-cnn-wins/"><![CDATA[<h2 id="the-experiment-how-many-layers-do-you-really-need">The Experiment: How Many Layers Do You Really Need?</h2> <p>For my <a href="/projects/image-generation/">Image Generation project</a>, before diving into GANs, I wanted to build a solid understanding of Convolutional Neural Networks (CNNs).</p> <p><strong>The task:</strong> Classify handwritten digits (MNIST dataset) - the “Hello World” of deep learning.</p> <p><strong>The question:</strong> How much architectural complexity do you actually need to get good results?</p> <p>I tested <strong>7 different architectures</strong>:</p> <ul> <li>3 traditional CNNs (varying depth and layer types)</li> <li>4 Fully Convolutional Networks (FCNNs)</li> </ul> <p><strong>The surprising result?</strong> The simplest model (CNN-1) achieved the best accuracy-to-training-time ratio.</p> <p>This post shares what I learned from systematically comparing these architectures.</p> <hr/> <h2 id="the-architectures">The Architectures</h2> <h3 id="cnn-family-traditional-convolutional-networks">CNN Family: Traditional Convolutional Networks</h3> <p>All CNNs follow this pattern:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Convolutional Layers → Pooling → Flatten → Fully Connected → Output
</code></pre></div></div> <h4 id="cnn-1-baseline">CNN-1 (Baseline)</h4> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~100K <strong>Training time:</strong> 12 minutes (100 epochs) <strong>Accuracy:</strong> <strong>98.0%</strong></p> <p><strong>Design philosophy:</strong> Keep it simple. Three conv layers, one fully connected layer.</p> <hr/> <h4 id="cnn-2-more-convolutional-layers">CNN-2 (More Convolutional Layers)</h4> <p><strong>Change:</strong> 3 conv layers → 5 conv layers</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~250K <strong>Training time:</strong> 19 minutes (+58% vs CNN-1) <strong>Accuracy:</strong> <strong>98.6%</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/CNN2-training-480.webp 480w,/assets/img/projects/image-generation/CNN2-training-800.webp 800w,/assets/img/projects/image-generation/CNN2-training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/CNN2-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CNN-2 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CNN-2 training curves showing overfitting starting around epoch 40 </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Best accuracy</strong> of all models (98.6%)</li> <li>⚠️ <strong>Overfitting</strong> starts around epoch 40 (validation loss increases)</li> <li>⚠️ <strong>60% longer training</strong> for only 0.6% accuracy gain</li> </ul> <p><strong>Is it worth it?</strong> Depends on your use case. If you need that extra 0.6%, yes. If not, CNN-1 is better.</p> <hr/> <h4 id="cnn-3-more-fully-connected-layers">CNN-3 (More Fully Connected Layers)</h4> <p><strong>Change:</strong> 1 FC layer → 3 FC layers</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~150K <strong>Training time:</strong> 13 minutes (+8% vs CNN-1) <strong>Accuracy:</strong> <strong>98.0%</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/CNN3-training-480.webp 480w,/assets/img/projects/image-generation/CNN3-training-800.webp 800w,/assets/img/projects/image-generation/CNN3-training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/CNN3-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CNN-3 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CNN-3 overfits earlier than CNN-2 (starts around epoch 15) </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Same accuracy</strong> as CNN-1 (98.0%)</li> <li>⚠️ <strong>Overfits earliest</strong> of all CNNs (epoch 15)</li> <li>⚠️ <strong>More parameters</strong> but no accuracy gain</li> </ul> <p><strong>Conclusion:</strong> Adding FC layers doesn’t help for this task.</p> <hr/> <h3 id="fcnn-family-fully-convolutional-networks">FCNN Family: Fully Convolutional Networks</h3> <p>FCNNs replace fully connected layers with global pooling:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Convolutional Layers → Global Average Pooling → Output
</code></pre></div></div> <p><strong>Why try FCNNs?</strong></p> <ul> <li>Fewer parameters (no dense layers)</li> <li>More robust to input size changes</li> <li>Popular in modern architectures (ResNet, EfficientNet)</li> </ul> <hr/> <h4 id="fcnn-1-baseline">FCNN-1 (Baseline)</h4> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">GlobalAveragePooling2D</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~50K (half of CNN-1) <strong>Training time:</strong> 11 minutes <strong>Accuracy:</strong> <strong>96.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>⚠️ <strong>2% worse</strong> than CNN-1 despite same conv structure</li> <li>✅ <strong>Fewer parameters</strong> (50% reduction)</li> <li>⚠️ <strong>Slower convergence</strong> (needs more epochs)</li> </ul> <p><strong>Why lower accuracy?</strong> Global pooling loses spatial information that FC layers preserve.</p> <hr/> <h4 id="fcnn-2-more-layers">FCNN-2 (More Layers)</h4> <p><strong>Change:</strong> 3 conv layers → 5 conv layers</p> <p><strong>Parameters:</strong> ~120K <strong>Training time:</strong> 15 minutes <strong>Accuracy:</strong> <strong>97.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Better than FCNN-1</strong> (adding depth helps)</li> <li>⚠️ <strong>Still worse than CNN-1</strong> (97% vs 98%)</li> </ul> <p><strong>Conclusion:</strong> More layers partially compensates for lack of FC layers, but doesn’t fully close the gap.</p> <hr/> <h4 id="fcnn-3-larger-kernels">FCNN-3 (Larger Kernels)</h4> <p><strong>Change:</strong> 3×3 kernels → 5×5 kernels</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>  <span class="c1"># Larger kernels
</span>    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">GlobalAveragePooling2D</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~85K <strong>Training time:</strong> 13 minutes <strong>Accuracy:</strong> <strong>97.5%</strong> (best FCNN!)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/FCNN-accuracy-comparison-480.webp 480w,/assets/img/projects/image-generation/FCNN-accuracy-comparison-800.webp 800w,/assets/img/projects/image-generation/FCNN-accuracy-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/FCNN-accuracy-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FCNN comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Accuracy comparison across all 4 FCNN architectures </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Best FCNN performance</strong> (97.5%)</li> <li>✅ <strong>Minimal training time increase</strong> (only +2 minutes vs FCNN-1)</li> <li>💡 <strong>Key insight:</strong> Kernel size matters more than depth for FCNNs</li> </ul> <p><strong>Why does this work?</strong> Larger kernels capture more spatial context, partially compensating for global pooling’s information loss.</p> <hr/> <h4 id="fcnn-4-more-layers--larger-kernels">FCNN-4 (More Layers + Larger Kernels)</h4> <p><strong>Change:</strong> Combine both improvements (5 layers + 5×5 kernels)</p> <p><strong>Parameters:</strong> ~150K <strong>Training time:</strong> 18 minutes <strong>Accuracy:</strong> <strong>97.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>❌ <strong>Worse than FCNN-3</strong> despite more complexity!</li> <li>⚠️ <strong>Overfitting</strong> starts around epoch 20</li> <li>⚠️ <strong>Diminishing returns</strong> from added complexity</li> </ul> <p><strong>Conclusion:</strong> More complexity doesn’t always help. FCNN-3 found the sweet spot.</p> <hr/> <h2 id="side-by-side-comparison">Side-by-Side Comparison</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-480.webp 480w,/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-800.webp 800w,/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/Accuracy-and-training-time-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="All models comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Accuracy vs training time for all 7 architectures. Top-left corner is best (high accuracy, low time). </div> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Type</strong></th> <th><strong>Params</strong></th> <th><strong>Time</strong></th> <th><strong>Accuracy</strong></th> <th><strong>Efficiency</strong></th> </tr> </thead> <tbody> <tr> <td><strong>CNN-1</strong></td> <td>Baseline</td> <td>100K</td> <td>12 min</td> <td><strong>98.0%</strong></td> <td>⭐⭐⭐⭐⭐</td> </tr> <tr> <td>CNN-2</td> <td>Deep CNN</td> <td>250K</td> <td>19 min</td> <td><strong>98.6%</strong></td> <td>⭐⭐⭐</td> </tr> <tr> <td>CNN-3</td> <td>Deep FC</td> <td>150K</td> <td>13 min</td> <td>98.0%</td> <td>⭐⭐⭐⭐</td> </tr> <tr> <td>FCNN-1</td> <td>Baseline</td> <td>50K</td> <td>11 min</td> <td>96.0%</td> <td>⭐⭐</td> </tr> <tr> <td>FCNN-2</td> <td>Deep</td> <td>120K</td> <td>15 min</td> <td>97.0%</td> <td>⭐⭐⭐</td> </tr> <tr> <td><strong>FCNN-3</strong></td> <td>Large kernels</td> <td>85K</td> <td>13 min</td> <td><strong>97.5%</strong></td> <td>⭐⭐⭐⭐</td> </tr> <tr> <td>FCNN-4</td> <td>Deep + Large</td> <td>150K</td> <td>18 min</td> <td>97.0%</td> <td>⭐⭐</td> </tr> </tbody> </table> <p><strong>Winner by accuracy:</strong> CNN-2 (98.6%) <strong>Winner by efficiency:</strong> <strong>CNN-1 (98.0% in 12 minutes)</strong> <strong>Best FCNN:</strong> FCNN-3 (97.5%)</p> <hr/> <h2 id="key-insights">Key Insights</h2> <h3 id="1-the-baseline-often-wins">1. The Baseline Often Wins</h3> <p>CNN-1 (the simplest model) achieved 98% accuracy—only 0.6% worse than the best model, but <strong>60% faster to train</strong>.</p> <p>For most applications, <strong>this tradeoff favors the baseline.</strong></p> <p><strong>When to use simple models:</strong></p> <ul> <li>Prototyping (iterate faster)</li> <li>Resource-constrained deployment (mobile, edge devices)</li> <li>“Good enough” accuracy (98% vs 98.6% doesn’t matter for many tasks)</li> </ul> <p><strong>When to invest in complexity:</strong></p> <ul> <li>Every 0.1% matters (medical diagnosis, safety-critical)</li> <li>Have abundant compute resources</li> <li>Need state-of-the-art results</li> </ul> <h3 id="2-adding-layers--better-results">2. Adding Layers ≠ Better Results</h3> <p><strong>Adding convolutional layers</strong> (CNN-2): ✅ Helped (+0.6% accuracy) <strong>Adding fully connected layers</strong> (CNN-3): ❌ No improvement (same 98%, more overfitting) <strong>Adding both to FCNN</strong> (FCNN-4): ❌ Made it worse (97.5% → 97.0%)</p> <p><strong>Lesson:</strong> More layers help when they address a bottleneck. Otherwise, they just overfit.</p> <h3 id="3-kernel-size-is-underrated">3. Kernel Size Is Underrated</h3> <p>FCNN-3 (larger kernels) outperformed FCNN-2 (more layers):</p> <ul> <li><strong>FCNN-2:</strong> 5 layers, 3×3 kernels → 97.0%</li> <li><strong>FCNN-3:</strong> 3 layers, 5×5 kernels → 97.5%</li> </ul> <p><strong>Why?</strong> Larger kernels capture more spatial context per layer. For small images (28×28), this matters.</p> <p><strong>Tradeoff:</strong> 5×5 kernels have ~2.8× more parameters than 3×3, but training time only increased 15%.</p> <h3 id="4-fcnns-need-more-capacity-than-cnns">4. FCNNs Need More Capacity Than CNNs</h3> <p>For the same convolutional structure:</p> <ul> <li><strong>CNN-1</strong> (with FC layers): 98.0%</li> <li><strong>FCNN-1</strong> (without FC layers): 96.0%</li> </ul> <p><strong>Why?</strong> Global pooling discards spatial information. FC layers recover some of it through dense connections.</p> <p><strong>To match CNN performance, FCNNs need:</strong></p> <ul> <li>More convolutional layers (FCNN-2)</li> <li>Larger kernels (FCNN-3)</li> <li>Or both (but watch for overfitting)</li> </ul> <h3 id="5-training-time-scales-nonlinearly">5. Training Time Scales Nonlinearly</h3> <table> <thead> <tr> <th><strong>Complexity</strong></th> <th><strong>Params</strong></th> <th><strong>Time</strong></th> <th><strong>Time per 1K Params</strong></th> </tr> </thead> <tbody> <tr> <td>FCNN-1</td> <td>50K</td> <td>11 min</td> <td>0.22 min</td> </tr> <tr> <td>CNN-1</td> <td>100K</td> <td>12 min</td> <td>0.12 min</td> </tr> <tr> <td>CNN-3</td> <td>150K</td> <td>13 min</td> <td>0.09 min</td> </tr> <tr> <td>CNN-2</td> <td>250K</td> <td>19 min</td> <td>0.08 min</td> </tr> </tbody> </table> <p><strong>Observation:</strong> Doubling parameters doesn’t double training time. Factors matter:</p> <ul> <li><strong>Layer type</strong> (conv vs dense)</li> <li><strong>Batch size</strong> (GPU utilization)</li> <li><strong>Data throughput</strong> (I/O bottlenecks)</li> </ul> <hr/> <h2 id="practical-recommendations">Practical Recommendations</h2> <h3 id="for-mnist-like-tasks-small-images-simple-patterns">For MNIST-Like Tasks (Small Images, Simple Patterns)</h3> <p><strong>Start with:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span> <span class="err">→</span> <span class="n">MaxPool</span> <span class="err">→</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">MaxPool</span> <span class="err">→</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">Flatten</span> <span class="err">→</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">Output</span>
</code></pre></div></div> <p>This is CNN-1. It’s fast, simple, and gets 98%.</p> <p><strong>Only add complexity if:</strong></p> <ul> <li>98% isn’t good enough</li> <li>You have compute budget to spare</li> <li>You’ve exhausted other improvements (data augmentation, regularization)</li> </ul> <h3 id="for-larger-images--complex-tasks">For Larger Images / Complex Tasks</h3> <p><strong>Modern best practices:</strong></p> <ul> <li>Use residual connections (ResNet)</li> <li>Batch normalization (not tested here, but helps)</li> <li>Data augmentation (critical for preventing overfitting)</li> <li>Transfer learning (if applicable)</li> </ul> <p>But the principle holds: <strong>Start simple, add complexity only when needed.</strong></p> <h3 id="for-resource-constrained-deployment">For Resource-Constrained Deployment</h3> <p><strong>Consider FCNNs:</strong></p> <ul> <li>Fewer parameters (FCNN-1: 50K vs CNN-1: 100K)</li> <li>No input size restrictions (can process variable-sized images)</li> <li>Faster inference on some hardware</li> </ul> <p><strong>But accept the accuracy tradeoff:</strong> ~1-2% worse than equivalent CNNs.</p> <hr/> <h2 id="what-i-wish-id-done-differently">What I Wish I’d Done Differently</h2> <h3 id="1-test-with-data-augmentation">1. Test with Data Augmentation</h3> <p>All models were tested <strong>without data augmentation</strong>. This likely led to:</p> <ul> <li>Earlier overfitting</li> <li>Lower final accuracy</li> </ul> <p><strong>Data augmentation would have:</strong></p> <ul> <li>Reduced overfitting (especially for CNN-2, CNN-3)</li> <li>Potentially closed the gap between complex and simple models</li> </ul> <h3 id="2-use-learning-rate-schedules">2. Use Learning Rate Schedules</h3> <p>I used a fixed learning rate (0.001). <strong>Learning rate decay</strong> would have:</p> <ul> <li>Improved final accuracy</li> <li>Helped models converge better</li> </ul> <h3 id="3-test-batch-normalization">3. Test Batch Normalization</h3> <p>Modern CNNs use batch normalization between conv layers. This would have:</p> <ul> <li>Allowed deeper models without vanishing gradients</li> <li>Potentially made CNN-2 train faster</li> <li>Reduced overfitting</li> </ul> <p><strong>Without BN, comparing “deep” models isn’t entirely fair.</strong></p> <h3 id="4-measure-inference-time-not-just-training-time">4. Measure Inference Time, Not Just Training Time</h3> <p>I focused on training time, but for deployment, <strong>inference time</strong> matters more.</p> <p><strong>Hypothesis:</strong> FCNNs might be faster at inference (no dense layers), even if training is similar.</p> <hr/> <h2 id="code--reproducibility">Code &amp; Reproducibility</h2> <p>All 7 architectures and training code in my <a href="https://github.com/LeonardoPaccianiMori/image-generation">Image Generation repository</a>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">notebooks/cnn_comparison.ipynb</code> - All experiments</li> <li><code class="language-plaintext highlighter-rouge">models/cnn_baseline.py</code> - CNN-1 implementation</li> <li><code class="language-plaintext highlighter-rouge">models/fcnn_variants.py</code> - FCNN implementations</li> </ul> <p><strong>To reproduce:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/LeonardoPaccianiMori/image-generation
<span class="nb">cd </span>image-generation
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
jupyter notebook notebooks/cnn_comparison.ipynb
</code></pre></div></div> <hr/> <h2 id="related-work">Related Work</h2> <p>This isn’t a new finding! Lots of research shows simpler models can match complex ones:</p> <ul> <li><strong>“Do We Need Hundreds of Classifiers?”</strong> (<a href="https://jmlr.org/papers/v15/delgado14a.html">Fernández-Delgado et al., 2014</a>) - Random Forests beat many complex models</li> <li><strong>“The Lottery Ticket Hypothesis”</strong> (<a href="https://arxiv.org/abs/1803.03635">Frankle &amp; Carbin, 2019</a>) - Large networks contain smaller subnetworks that perform equally well</li> <li><strong>“Rethinking the Inception Architecture”</strong> (<a href="https://arxiv.org/abs/1512.00567">Szegedy et al., 2016</a>) - Factorized convolutions (smaller kernels) can be more efficient</li> </ul> <p><strong>My contribution:</strong> Systematic comparison on a specific task (MNIST) with practical insights for practitioners.</p> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li><strong>Start with simple baselines</strong> - CNN-1 (98% in 12 min) beats complex models in efficiency</li> <li><strong>More layers ≠ better results</strong> - CNN-3 had 3 FC layers but same accuracy as 1 FC layer</li> <li><strong>Kernel size matters</strong> - FCNN-3 (5×5 kernels) beat FCNN-2 (more layers) with less training time</li> <li><strong>Measure what matters</strong> - Accuracy alone doesn’t tell the story; consider training time, parameters, overfitting</li> <li><strong>Complex models need careful tuning</strong> - Without data augmentation and proper regularization, they just overfit</li> <li><strong>FCNNs trade accuracy for simplicity</strong> - 1-2% worse than CNNs, but fewer parameters</li> </ol> <p><strong>The meta-lesson:</strong> Don’t default to complexity. Simple models, properly trained, often win.</p> <p>Have you been surprised by a simple model outperforming a complex one? What’s your approach to choosing architectures? Let me know!</p>]]></content><author><name></name></author><category term="data-science"/><category term="deep-learning"/><category term="CNN"/><category term="optimization"/><summary type="html"><![CDATA[Testing 7 different neural network architectures taught me that more layers doesn't mean better results—sometimes the baseline is the best choice]]></summary></entry><entry><title type="html">Activation Functions Killed My GAN: A Debugging Story</title><link href="https://unique-cocada-065d88.netlify.app//blog/2025/gan-activation-functions/" rel="alternate" type="text/html" title="Activation Functions Killed My GAN: A Debugging Story"/><published>2025-01-10T14:00:00+00:00</published><updated>2025-01-10T14:00:00+00:00</updated><id>https://unique-cocada-065d88.netlify.app//blog/2025/gan-activation-functions</id><content type="html" xml:base="https://unique-cocada-065d88.netlify.app//blog/2025/gan-activation-functions/"><![CDATA[<h2 id="the-goal-generate-handwritten-digits-with-gans">The Goal: Generate Handwritten Digits with GANs</h2> <p>For my <a href="/projects/image-generation/">Image Generation project</a>, I wanted to train a Deep Convolutional GAN (DCGAN) to generate realistic handwritten digits from the MNIST dataset.</p> <p><strong>The architecture</strong> was straightforward:</p> <ul> <li><strong>Generator</strong>: Takes random noise → Produces 28×28 digit images</li> <li><strong>Discriminator</strong>: Takes images → Classifies as real or fake</li> <li><strong>Training</strong>: Adversarial process where both networks improve</li> </ul> <p>I followed the standard DCGAN architecture from the <a href="https://arxiv.org/abs/1511.06434">original paper</a>, implemented it carefully in TensorFlow/Keras, and hit train.</p> <p><strong>The result?</strong> Complete garbage.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN1-examples-480.webp 480w,/assets/img/projects/image-generation/DCGAN1-examples-800.webp 800w,/assets/img/projects/image-generation/DCGAN1-examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN1-examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-1 failed outputs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-1 results: Only vague shapes resembling 0s and 9s. Everything else is noise. </div> <p>After 100 epochs of training, the generator could <em>barely</em> produce recognizable 0s and 9s. Everything else looked like static.</p> <p><strong>This post is the story of how I debugged this failure</strong> and discovered that a single activation function choice was sabotaging everything.</p> <hr/> <h2 id="dcgan-1-the-by-the-book-implementation">DCGAN-1: The “By-the-Book” Implementation</h2> <h3 id="architecture">Architecture</h3> <p>I followed the DCGAN guidelines religiously:</p> <p><strong>Generator:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_generator</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
        <span class="nc">Dense</span><span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
        <span class="nc">Reshape</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>

        <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
        <span class="nc">BatchNormalization</span><span class="p">(),</span>
        <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>

        <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
        <span class="nc">BatchNormalization</span><span class="p">(),</span>
        <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>

        <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
        <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># ← THE PROBLEM WAS HERE
</span>    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p><strong>Discriminator:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_discriminator</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
        <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
        <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>

        <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
        <span class="nc">BatchNormalization</span><span class="p">(),</span>
        <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>

        <span class="nc">Flatten</span><span class="p">(),</span>
        <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p><strong>Training setup:</strong></p> <ul> <li>Optimizer: Adam (lr=0.0002, beta_1=0.5)</li> <li>Loss: Binary crossentropy</li> <li>Batch size: 128</li> <li>Epochs: 100</li> </ul> <p>All standard choices from DCGAN papers and tutorials.</p> <h3 id="training-behavior-red-flags-everywhere">Training Behavior: Red Flags Everywhere</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN1-training-480.webp 480w,/assets/img/projects/image-generation/DCGAN1-training-800.webp 800w,/assets/img/projects/image-generation/DCGAN1-training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN1-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-1 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-1 training curves showing discriminator plateauing and generator loss diverging. </div> <p><strong>What I observed:</strong></p> <ul> <li><strong>Discriminator loss</strong>: Quickly dropped to ~0.1 and plateaued</li> <li><strong>Generator loss</strong>: Steadily increased throughout training</li> <li><strong>Generated images</strong>: Barely improved after epoch 20</li> </ul> <p><strong>Classic signs of:</strong></p> <ul> <li>Generator couldn’t fool discriminator at all</li> <li>Discriminator became “too good” too fast</li> <li>Training collapsed into a failure mode</li> </ul> <h3 id="initial-hypotheses-all-wrong">Initial Hypotheses (All Wrong)</h3> <p>I tried everything I could think of:</p> <ol> <li><strong>Learning rate tweaking</strong> <ul> <li>Tried 0.0001, 0.00005, 0.0003</li> <li>Result: No meaningful improvement</li> </ul> </li> <li><strong>Label smoothing</strong> <ul> <li>Used 0.9 instead of 1.0 for “real” labels</li> <li>Result: Slightly more stable, still terrible output</li> </ul> </li> <li><strong>Discriminator regularization</strong> <ul> <li>Added dropout</li> <li>Reduced discriminator training steps (1 D update per 2 G updates)</li> <li>Result: Marginally better, but fundamentally broken</li> </ul> </li> <li><strong>More data augmentation</strong> <ul> <li>Random shifts, rotations</li> <li>Result: No change</li> </ul> </li> <li><strong>Architecture changes</strong> <ul> <li>More/fewer filters</li> <li>Different kernel sizes</li> <li>Result: Same problem persists</li> </ul> </li> </ol> <p><strong>Nothing worked.</strong> I was stuck with a GAN that could barely generate two digits out of ten.</p> <hr/> <h2 id="the-breakthrough-one-line-of-code">The Breakthrough: One Line of Code</h2> <p>After days of frustration, I decided to question <strong>every single architectural choice</strong>, including the ones that seemed “obviously correct.”</p> <p>One thing caught my eye: <strong>Why <code class="language-plaintext highlighter-rouge">tanh</code> activation in the generator output?</strong></p> <p><strong>The standard reasoning:</strong></p> <ul> <li>DCGAN paper uses <code class="language-plaintext highlighter-rouge">tanh</code></li> <li><code class="language-plaintext highlighter-rouge">tanh</code> outputs [-1, 1] range</li> <li>MNIST images are normalized to [-1, 1]</li> <li>Seems logical, right?</li> </ul> <p>But then I thought: <strong>What if the problem is actually the activation function?</strong></p> <h3 id="dcgan-2-the-one-line-change">DCGAN-2: The One-Line Change</h3> <p>I changed <strong>exactly one line</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before (DCGAN-1)
</span><span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
<span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># After (DCGAN-2)
</span><span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
<span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># ← CHANGED THIS
</span></code></pre></div></div> <p>And re-normalized the input images to [0, 1] instead of [-1, 1].</p> <p><strong>Everything else stayed the same</strong>: architecture, optimizer, hyperparameters, training procedure.</p> <h3 id="the-results-night-and-day-difference">The Results: Night and Day Difference</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN2-training-480.webp 480w,/assets/img/projects/image-generation/DCGAN2-training-800.webp 800w,/assets/img/projects/image-generation/DCGAN2-training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN2-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-2 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-2 training curves showing healthy oscillation between generator and discriminator losses. </div> <p><strong>Training behavior:</strong></p> <ul> <li>✅ <strong>Losses oscillate healthily</strong> - Neither network dominates</li> <li>✅ <strong>Generator loss stays bounded</strong> - No divergence</li> <li>✅ <strong>Discriminator challenged</strong> - Accuracy hovers around 70-80%</li> <li>✅ <strong>Visible improvement</strong> - Images get better every 10 epochs</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN2-examples-480.webp 480w,/assets/img/projects/image-generation/DCGAN2-examples-800.webp 800w,/assets/img/projects/image-generation/DCGAN2-examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN2-examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-2 outputs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-2 results: Most digits (0-9) are now recognizable! Not perfect, but a massive improvement. </div> <p><strong>Generated images:</strong></p> <ul> <li>✅ All 10 digits visible (0-9)</li> <li>✅ Recognizable handwritten style</li> <li>⚠️ Still some artifacts and unclear digits</li> <li>⚠️ Quality varies across different digits</li> </ul> <p><strong>Bottom line:</strong> Changing <code class="language-plaintext highlighter-rouge">tanh</code> → <code class="language-plaintext highlighter-rouge">sigmoid</code> transformed a complete failure into a working GAN.</p> <hr/> <h2 id="why-did-this-work-the-technical-explanation">Why Did This Work? The Technical Explanation</h2> <h3 id="the-problem-with-tanh">The Problem with Tanh</h3> <p><code class="language-plaintext highlighter-rouge">tanh</code> activation outputs values in <strong>[-1, 1]</strong>.</p> <p><strong>Issues for image generation:</strong></p> <ol> <li><strong>Saturation at extremes</strong> <ul> <li>For very dark pixels (0 in [0, 1] scale), generator needs to output -1</li> <li>For very bright pixels (1 in [0, 1] scale), generator needs to output +1</li> <li>Both are at the extremes where <code class="language-plaintext highlighter-rouge">tanh</code> gradient vanishes</li> </ul> </li> <li><strong>Gradient flow problems</strong> <ul> <li>When generator output saturates (near -1 or +1), gradient ≈ 0</li> <li>Generator can’t learn effectively</li> <li>Discriminator wins easily because generator is “stuck”</li> </ul> </li> <li><strong>Training dynamics</strong> <ul> <li>Early in training, generator outputs random noise around 0</li> <li>To produce valid images, needs to push outputs to extremes (-1 or +1)</li> <li>This is hard because of saturation issues</li> <li>Generator gets “trapped” producing mediocre outputs</li> </ul> </li> </ol> <h3 id="why-sigmoid-works-better">Why Sigmoid Works Better</h3> <p><code class="language-plaintext highlighter-rouge">sigmoid</code> activation outputs values in <strong>[0, 1]</strong>.</p> <p><strong>Advantages for this task:</strong></p> <ol> <li><strong>Direct mapping to pixel values</strong> <ul> <li>Output 0.0 → Black pixel (0)</li> <li>Output 1.0 → White pixel (255 after scaling)</li> <li>No need for range transformation</li> </ul> </li> <li><strong>Better gradient flow</strong> <ul> <li>Black pixels (0) don’t require extreme saturation</li> <li>Generator can comfortably output 0.01 (nearly black) with good gradients</li> <li>White pixels (1) similarly accessible</li> </ul> </li> <li><strong>Easier optimization</strong> <ul> <li>Generator can make incremental progress</li> <li>Discriminator doesn’t immediately dominate</li> <li>Training reaches equilibrium naturally</li> </ul> </li> </ol> <h3 id="a-note-on-best-practices">A Note on “Best Practices”</h3> <p><strong>Why do DCGAN papers use <code class="language-plaintext highlighter-rouge">tanh</code>?</strong></p> <p>Many DCGAN implementations use <code class="language-plaintext highlighter-rouge">tanh</code> because:</p> <ul> <li>The original DCGAN paper used it</li> <li>Works well for some datasets (ImageNet, CelebA with careful tuning)</li> <li>Historical momentum (everyone copies it)</li> </ul> <p><strong>But it’s not universal.</strong></p> <p>For MNIST specifically, with its high-contrast black-and-white images, <code class="language-plaintext highlighter-rouge">sigmoid</code> is actually better suited.</p> <p><strong>Lesson:</strong> Question “best practices” when they don’t work for your specific problem.</p> <hr/> <h2 id="further-improvements-dcgan-3-and-dcgan-4">Further Improvements: DCGAN-3 and DCGAN-4</h2> <p>After fixing the activation function, I made two more improvements:</p> <h3 id="dcgan-3-larger-kernels-33--55">DCGAN-3: Larger Kernels (3×3 → 5×5)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>  <span class="c1"># Was (3, 3)
</span></code></pre></div></div> <p><strong>Result:</strong></p> <ul> <li>✅ <strong>Smoother images</strong> - Less pixelation</li> <li>✅ <strong>Better digit quality</strong> - All 10 digits clearly visible</li> <li>✅ <strong>Training time</strong> - Only ~10% longer</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN3-examples-480.webp 480w,/assets/img/projects/image-generation/DCGAN3-examples-800.webp 800w,/assets/img/projects/image-generation/DCGAN3-examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN3-examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-3 outputs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-3 with 5×5 kernels: Noticeably smoother and more realistic digits. </div> <h3 id="dcgan-4-one-more-convolutional-layer">DCGAN-4: One More Convolutional Layer</h3> <p>Added a 4th convolutional layer to both generator and discriminator:</p> <p><strong>Result:</strong></p> <ul> <li>✅ <strong>Best quality yet</strong> - Sharpest, most realistic digits</li> <li>✅ <strong>Fewer artifacts</strong> - Cleaner backgrounds</li> <li>⚠️ <strong>Longer training</strong> - ~40% more time per epoch</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/DCGAN4-examples-480.webp 480w,/assets/img/projects/image-generation/DCGAN4-examples-800.webp 800w,/assets/img/projects/image-generation/DCGAN4-examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/projects/image-generation/DCGAN4-examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DCGAN-4 outputs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DCGAN-4 (final model): 100 generated digits showing consistent quality across all numbers 0-9. </div> <hr/> <h2 id="side-by-side-comparison">Side-by-Side Comparison</h2> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Key Change</strong></th> <th><strong>Digits Recognizable</strong></th> <th><strong>Quality Rating</strong></th> <th><strong>Training Time</strong></th> </tr> </thead> <tbody> <tr> <td>DCGAN-1</td> <td>Baseline (tanh)</td> <td>2/10 (only 0, 9)</td> <td>⭐</td> <td>100% (baseline)</td> </tr> <tr> <td>DCGAN-2</td> <td><strong>tanh → sigmoid</strong></td> <td>10/10</td> <td>⭐⭐⭐</td> <td>100%</td> </tr> <tr> <td>DCGAN-3</td> <td>Larger kernels</td> <td>10/10</td> <td>⭐⭐⭐⭐</td> <td>110%</td> </tr> <tr> <td>DCGAN-4</td> <td>+1 conv layer</td> <td>10/10</td> <td>⭐⭐⭐⭐⭐</td> <td>140%</td> </tr> </tbody> </table> <p><strong>The activation function change (DCGAN-1 → DCGAN-2) was the critical breakthrough.</strong> Everything else was incremental improvement.</p> <hr/> <h2 id="lessons-learned">Lessons Learned</h2> <h3 id="1-one-line-of-code-can-make-or-break-your-model">1. One Line of Code Can Make or Break Your Model</h3> <p>I spent days tweaking hyperparameters, adjusting learning rates, modifying architectures. <strong>None of it mattered</strong> until I fixed the activation function.</p> <p><strong>Takeaway:</strong> When debugging, don’t just tune; question fundamental architectural choices.</p> <h3 id="2-best-practices-are-context-dependent">2. “Best Practices” Are Context-Dependent</h3> <p>Using <code class="language-plaintext highlighter-rouge">tanh</code> for GAN generators is common advice, but it wasn’t right for my specific task (MNIST digit generation).</p> <p><strong>Takeaway:</strong> Understand <em>why</em> a best practice exists, not just <em>what</em> it is. Adapt to your problem.</p> <h3 id="3-training-curves-tell-a-story">3. Training Curves Tell a Story</h3> <p>The DCGAN-1 training curves clearly showed:</p> <ul> <li>Discriminator plateauing (too easy)</li> <li>Generator loss diverging (can’t improve)</li> </ul> <p>I should have recognized this pattern earlier as <strong>a fundamental architecture problem</strong>, not just a hyperparameter issue.</p> <p><strong>Takeaway:</strong> Learn to read training curves. Divergence often means architectural mismatch.</p> <h3 id="4-systematic-experimentation-pays-off">4. Systematic Experimentation Pays Off</h3> <p>After the initial failures, I created a methodical testing framework:</p> <ol> <li>Change ONE thing at a time</li> <li>Train for 100 epochs</li> <li>Compare training curves + generated samples</li> <li>Document results</li> </ol> <p><strong>This process led me to the solution.</strong></p> <p><strong>Takeaway:</strong> When stuck, slow down and experiment systematically.</p> <h3 id="5-gans-are-sensitive">5. GANs Are Sensitive</h3> <p>GANs are notoriously finicky. Small changes can have dramatic effects:</p> <ul> <li>Activation function: Failure → Success</li> <li>Kernel size: OK → Great</li> <li>One more layer: Great → Excellent</li> </ul> <p><strong>Takeaway:</strong> Don’t give up on GANs too quickly. A small tweak might unlock everything.</p> <hr/> <h2 id="code--resources">Code &amp; Resources</h2> <p>Full implementation in my <a href="https://github.com/LeonardoPaccianiMori/image-generation">Image Generation repository</a>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">notebooks/dcgan_experiments.ipynb</code> - All 4 DCGAN variations</li> <li><code class="language-plaintext highlighter-rouge">models/dcgan_final.py</code> - Best-performing model (DCGAN-4)</li> </ul> <p><strong>Related posts:</strong></p> <ul> <li><a href="/projects/image-generation/">Full Image Generation Project</a> - Context and other models (VAEs, CNNs)</li> </ul> <p><strong>Recommended reading:</strong></p> <ul> <li><a href="https://arxiv.org/abs/1511.06434">Original DCGAN Paper</a></li> <li><a href="https://github.com/soumith/ganhacks">GAN Training Tips</a> - Community best practices</li> </ul> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li><strong>Activation functions matter more than you think</strong> - One change (tanh → sigmoid) can transform results</li> <li><strong>Debug systematically</strong> - Change one variable at a time, document everything</li> <li><strong>Training curves reveal architectural problems</strong> - Learn to recognize unhealthy patterns</li> <li><strong>Question “best practices”</strong> - What works for ImageNet might not work for MNIST</li> <li><strong>Small architectural tweaks compound</strong> - Activation + kernels + depth = big improvement</li> </ol> <p>Have you debugged a stubborn model and found a surprising fix? What was your “one line of code” moment? Let me know in the comments!</p> <hr/> <h2 id="appendix-full-training-parameters">Appendix: Full Training Parameters</h2> <p>For reproducibility, here are the exact settings for DCGAN-4 (final model):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Architecture
</span><span class="n">generator_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">discriminator_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Training
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Data preprocessing
</span><span class="n">image_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Sigmoid output
</span><span class="n">normalization</span> <span class="o">=</span> <span class="sh">'</span><span class="s">min-max</span><span class="sh">'</span>  <span class="c1"># Not mean-std
</span>
<span class="c1"># Activation functions
</span><span class="n">generator_output</span> <span class="o">=</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span>  <span class="c1"># KEY CHOICE
</span><span class="n">generator_hidden</span> <span class="o">=</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">discriminator_hidden</span> <span class="o">=</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">discriminator_output</span> <span class="o">=</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="data-science"/><category term="deep-learning"/><category term="GAN"/><category term="debugging"/><summary type="html"><![CDATA[How changing one line of code (tanh → sigmoid) took my GAN from complete failure to generating realistic digits]]></summary></entry></feed>