<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> When Simpler Models Win: Comparing 7 CNN Architectures | Pinco Pallino </title> <meta name="author" content="Pinco Pallino"> <meta name="description" content="Testing 7 different neural network architectures taught me that more layers doesn't mean better results—sometimes the baseline is the best choice"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&amp;family=IBM+Plex+Serif:wght@300;400;500;600;700&amp;family=Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unique-cocada-065d88.netlify.app//blog/2025/simple-cnn-wins/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pinco</span> Pallino </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">When Simpler Models Win: Comparing 7 CNN Architectures</h1> <p class="post-meta"> Created on February 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/cnn"> <i class="fa-solid fa-hashtag fa-sm"></i> CNN</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   ·   <a href="/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="the-experiment-how-many-layers-do-you-really-need">The Experiment: How Many Layers Do You Really Need?</h2> <p>For my <a href="/projects/image-generation/">Image Generation project</a>, before diving into GANs, I wanted to build a solid understanding of Convolutional Neural Networks (CNNs).</p> <p><strong>The task:</strong> Classify handwritten digits (MNIST dataset) - the “Hello World” of deep learning.</p> <p><strong>The question:</strong> How much architectural complexity do you actually need to get good results?</p> <p>I tested <strong>7 different architectures</strong>:</p> <ul> <li>3 traditional CNNs (varying depth and layer types)</li> <li>4 Fully Convolutional Networks (FCNNs)</li> </ul> <p><strong>The surprising result?</strong> The simplest model (CNN-1) achieved the best accuracy-to-training-time ratio.</p> <p>This post shares what I learned from systematically comparing these architectures.</p> <hr> <h2 id="the-architectures">The Architectures</h2> <h3 id="cnn-family-traditional-convolutional-networks">CNN Family: Traditional Convolutional Networks</h3> <p>All CNNs follow this pattern:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Convolutional Layers → Pooling → Flatten → Fully Connected → Output
</code></pre></div></div> <h4 id="cnn-1-baseline">CNN-1 (Baseline)</h4> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~100K <strong>Training time:</strong> 12 minutes (100 epochs) <strong>Accuracy:</strong> <strong>98.0%</strong></p> <p><strong>Design philosophy:</strong> Keep it simple. Three conv layers, one fully connected layer.</p> <hr> <h4 id="cnn-2-more-convolutional-layers">CNN-2 (More Convolutional Layers)</h4> <p><strong>Change:</strong> 3 conv layers → 5 conv layers</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~250K <strong>Training time:</strong> 19 minutes (+58% vs CNN-1) <strong>Accuracy:</strong> <strong>98.6%</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/CNN2-training-480.webp 480w,/assets/img/projects/image-generation/CNN2-training-800.webp 800w,/assets/img/projects/image-generation/CNN2-training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/image-generation/CNN2-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CNN-2 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> CNN-2 training curves showing overfitting starting around epoch 40 </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Best accuracy</strong> of all models (98.6%)</li> <li>⚠️ <strong>Overfitting</strong> starts around epoch 40 (validation loss increases)</li> <li>⚠️ <strong>60% longer training</strong> for only 0.6% accuracy gain</li> </ul> <p><strong>Is it worth it?</strong> Depends on your use case. If you need that extra 0.6%, yes. If not, CNN-1 is better.</p> <hr> <h4 id="cnn-3-more-fully-connected-layers">CNN-3 (More Fully Connected Layers)</h4> <p><strong>Change:</strong> 1 FC layer → 3 FC layers</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~150K <strong>Training time:</strong> 13 minutes (+8% vs CNN-1) <strong>Accuracy:</strong> <strong>98.0%</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/CNN3-training-480.webp 480w,/assets/img/projects/image-generation/CNN3-training-800.webp 800w,/assets/img/projects/image-generation/CNN3-training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/image-generation/CNN3-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CNN-3 training curves" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> CNN-3 overfits earlier than CNN-2 (starts around epoch 15) </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Same accuracy</strong> as CNN-1 (98.0%)</li> <li>⚠️ <strong>Overfits earliest</strong> of all CNNs (epoch 15)</li> <li>⚠️ <strong>More parameters</strong> but no accuracy gain</li> </ul> <p><strong>Conclusion:</strong> Adding FC layers doesn’t help for this task.</p> <hr> <h3 id="fcnn-family-fully-convolutional-networks">FCNN Family: Fully Convolutional Networks</h3> <p>FCNNs replace fully connected layers with global pooling:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Convolutional Layers → Global Average Pooling → Output
</code></pre></div></div> <p><strong>Why try FCNNs?</strong></p> <ul> <li>Fewer parameters (no dense layers)</li> <li>More robust to input size changes</li> <li>Popular in modern architectures (ResNet, EfficientNet)</li> </ul> <hr> <h4 id="fcnn-1-baseline">FCNN-1 (Baseline)</h4> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">GlobalAveragePooling2D</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~50K (half of CNN-1) <strong>Training time:</strong> 11 minutes <strong>Accuracy:</strong> <strong>96.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>⚠️ <strong>2% worse</strong> than CNN-1 despite same conv structure</li> <li>✅ <strong>Fewer parameters</strong> (50% reduction)</li> <li>⚠️ <strong>Slower convergence</strong> (needs more epochs)</li> </ul> <p><strong>Why lower accuracy?</strong> Global pooling loses spatial information that FC layers preserve.</p> <hr> <h4 id="fcnn-2-more-layers">FCNN-2 (More Layers)</h4> <p><strong>Change:</strong> 3 conv layers → 5 conv layers</p> <p><strong>Parameters:</strong> ~120K <strong>Training time:</strong> 15 minutes <strong>Accuracy:</strong> <strong>97.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Better than FCNN-1</strong> (adding depth helps)</li> <li>⚠️ <strong>Still worse than CNN-1</strong> (97% vs 98%)</li> </ul> <p><strong>Conclusion:</strong> More layers partially compensates for lack of FC layers, but doesn’t fully close the gap.</p> <hr> <h4 id="fcnn-3-larger-kernels">FCNN-3 (Larger Kernels)</h4> <p><strong>Change:</strong> 3×3 kernels → 5×5 kernels</p> <p><strong>Architecture:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>  <span class="c1"># Larger kernels
</span>    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span>

    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

    <span class="nc">GlobalAveragePooling2D</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p><strong>Parameters:</strong> ~85K <strong>Training time:</strong> 13 minutes <strong>Accuracy:</strong> <strong>97.5%</strong> (best FCNN!)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/FCNN-accuracy-comparison-480.webp 480w,/assets/img/projects/image-generation/FCNN-accuracy-comparison-800.webp 800w,/assets/img/projects/image-generation/FCNN-accuracy-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/image-generation/FCNN-accuracy-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FCNN comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Accuracy comparison across all 4 FCNN architectures </div> <p><strong>Observations:</strong></p> <ul> <li>✅ <strong>Best FCNN performance</strong> (97.5%)</li> <li>✅ <strong>Minimal training time increase</strong> (only +2 minutes vs FCNN-1)</li> <li>💡 <strong>Key insight:</strong> Kernel size matters more than depth for FCNNs</li> </ul> <p><strong>Why does this work?</strong> Larger kernels capture more spatial context, partially compensating for global pooling’s information loss.</p> <hr> <h4 id="fcnn-4-more-layers--larger-kernels">FCNN-4 (More Layers + Larger Kernels)</h4> <p><strong>Change:</strong> Combine both improvements (5 layers + 5×5 kernels)</p> <p><strong>Parameters:</strong> ~150K <strong>Training time:</strong> 18 minutes <strong>Accuracy:</strong> <strong>97.0%</strong></p> <p><strong>Observations:</strong></p> <ul> <li>❌ <strong>Worse than FCNN-3</strong> despite more complexity!</li> <li>⚠️ <strong>Overfitting</strong> starts around epoch 20</li> <li>⚠️ <strong>Diminishing returns</strong> from added complexity</li> </ul> <p><strong>Conclusion:</strong> More complexity doesn’t always help. FCNN-3 found the sweet spot.</p> <hr> <h2 id="side-by-side-comparison">Side-by-Side Comparison</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-480.webp 480w,/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-800.webp 800w,/assets/img/projects/image-generation/Accuracy-and-training-time-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/image-generation/Accuracy-and-training-time-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="All models comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Accuracy vs training time for all 7 architectures. Top-left corner is best (high accuracy, low time). </div> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Type</strong></th> <th><strong>Params</strong></th> <th><strong>Time</strong></th> <th><strong>Accuracy</strong></th> <th><strong>Efficiency</strong></th> </tr> </thead> <tbody> <tr> <td><strong>CNN-1</strong></td> <td>Baseline</td> <td>100K</td> <td>12 min</td> <td><strong>98.0%</strong></td> <td>⭐⭐⭐⭐⭐</td> </tr> <tr> <td>CNN-2</td> <td>Deep CNN</td> <td>250K</td> <td>19 min</td> <td><strong>98.6%</strong></td> <td>⭐⭐⭐</td> </tr> <tr> <td>CNN-3</td> <td>Deep FC</td> <td>150K</td> <td>13 min</td> <td>98.0%</td> <td>⭐⭐⭐⭐</td> </tr> <tr> <td>FCNN-1</td> <td>Baseline</td> <td>50K</td> <td>11 min</td> <td>96.0%</td> <td>⭐⭐</td> </tr> <tr> <td>FCNN-2</td> <td>Deep</td> <td>120K</td> <td>15 min</td> <td>97.0%</td> <td>⭐⭐⭐</td> </tr> <tr> <td><strong>FCNN-3</strong></td> <td>Large kernels</td> <td>85K</td> <td>13 min</td> <td><strong>97.5%</strong></td> <td>⭐⭐⭐⭐</td> </tr> <tr> <td>FCNN-4</td> <td>Deep + Large</td> <td>150K</td> <td>18 min</td> <td>97.0%</td> <td>⭐⭐</td> </tr> </tbody> </table> <p><strong>Winner by accuracy:</strong> CNN-2 (98.6%) <strong>Winner by efficiency:</strong> <strong>CNN-1 (98.0% in 12 minutes)</strong> <strong>Best FCNN:</strong> FCNN-3 (97.5%)</p> <hr> <h2 id="key-insights">Key Insights</h2> <h3 id="1-the-baseline-often-wins">1. The Baseline Often Wins</h3> <p>CNN-1 (the simplest model) achieved 98% accuracy—only 0.6% worse than the best model, but <strong>60% faster to train</strong>.</p> <p>For most applications, <strong>this tradeoff favors the baseline.</strong></p> <p><strong>When to use simple models:</strong></p> <ul> <li>Prototyping (iterate faster)</li> <li>Resource-constrained deployment (mobile, edge devices)</li> <li>“Good enough” accuracy (98% vs 98.6% doesn’t matter for many tasks)</li> </ul> <p><strong>When to invest in complexity:</strong></p> <ul> <li>Every 0.1% matters (medical diagnosis, safety-critical)</li> <li>Have abundant compute resources</li> <li>Need state-of-the-art results</li> </ul> <h3 id="2-adding-layers--better-results">2. Adding Layers ≠ Better Results</h3> <p><strong>Adding convolutional layers</strong> (CNN-2): ✅ Helped (+0.6% accuracy) <strong>Adding fully connected layers</strong> (CNN-3): ❌ No improvement (same 98%, more overfitting) <strong>Adding both to FCNN</strong> (FCNN-4): ❌ Made it worse (97.5% → 97.0%)</p> <p><strong>Lesson:</strong> More layers help when they address a bottleneck. Otherwise, they just overfit.</p> <h3 id="3-kernel-size-is-underrated">3. Kernel Size Is Underrated</h3> <p>FCNN-3 (larger kernels) outperformed FCNN-2 (more layers):</p> <ul> <li> <strong>FCNN-2:</strong> 5 layers, 3×3 kernels → 97.0%</li> <li> <strong>FCNN-3:</strong> 3 layers, 5×5 kernels → 97.5%</li> </ul> <p><strong>Why?</strong> Larger kernels capture more spatial context per layer. For small images (28×28), this matters.</p> <p><strong>Tradeoff:</strong> 5×5 kernels have ~2.8× more parameters than 3×3, but training time only increased 15%.</p> <h3 id="4-fcnns-need-more-capacity-than-cnns">4. FCNNs Need More Capacity Than CNNs</h3> <p>For the same convolutional structure:</p> <ul> <li> <strong>CNN-1</strong> (with FC layers): 98.0%</li> <li> <strong>FCNN-1</strong> (without FC layers): 96.0%</li> </ul> <p><strong>Why?</strong> Global pooling discards spatial information. FC layers recover some of it through dense connections.</p> <p><strong>To match CNN performance, FCNNs need:</strong></p> <ul> <li>More convolutional layers (FCNN-2)</li> <li>Larger kernels (FCNN-3)</li> <li>Or both (but watch for overfitting)</li> </ul> <h3 id="5-training-time-scales-nonlinearly">5. Training Time Scales Nonlinearly</h3> <table> <thead> <tr> <th><strong>Complexity</strong></th> <th><strong>Params</strong></th> <th><strong>Time</strong></th> <th><strong>Time per 1K Params</strong></th> </tr> </thead> <tbody> <tr> <td>FCNN-1</td> <td>50K</td> <td>11 min</td> <td>0.22 min</td> </tr> <tr> <td>CNN-1</td> <td>100K</td> <td>12 min</td> <td>0.12 min</td> </tr> <tr> <td>CNN-3</td> <td>150K</td> <td>13 min</td> <td>0.09 min</td> </tr> <tr> <td>CNN-2</td> <td>250K</td> <td>19 min</td> <td>0.08 min</td> </tr> </tbody> </table> <p><strong>Observation:</strong> Doubling parameters doesn’t double training time. Factors matter:</p> <ul> <li> <strong>Layer type</strong> (conv vs dense)</li> <li> <strong>Batch size</strong> (GPU utilization)</li> <li> <strong>Data throughput</strong> (I/O bottlenecks)</li> </ul> <hr> <h2 id="practical-recommendations">Practical Recommendations</h2> <h3 id="for-mnist-like-tasks-small-images-simple-patterns">For MNIST-Like Tasks (Small Images, Simple Patterns)</h3> <p><strong>Start with:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span> <span class="err">→</span> <span class="n">MaxPool</span> <span class="err">→</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">MaxPool</span> <span class="err">→</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">Flatten</span> <span class="err">→</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="err">→</span> <span class="n">Output</span>
</code></pre></div></div> <p>This is CNN-1. It’s fast, simple, and gets 98%.</p> <p><strong>Only add complexity if:</strong></p> <ul> <li>98% isn’t good enough</li> <li>You have compute budget to spare</li> <li>You’ve exhausted other improvements (data augmentation, regularization)</li> </ul> <h3 id="for-larger-images--complex-tasks">For Larger Images / Complex Tasks</h3> <p><strong>Modern best practices:</strong></p> <ul> <li>Use residual connections (ResNet)</li> <li>Batch normalization (not tested here, but helps)</li> <li>Data augmentation (critical for preventing overfitting)</li> <li>Transfer learning (if applicable)</li> </ul> <p>But the principle holds: <strong>Start simple, add complexity only when needed.</strong></p> <h3 id="for-resource-constrained-deployment">For Resource-Constrained Deployment</h3> <p><strong>Consider FCNNs:</strong></p> <ul> <li>Fewer parameters (FCNN-1: 50K vs CNN-1: 100K)</li> <li>No input size restrictions (can process variable-sized images)</li> <li>Faster inference on some hardware</li> </ul> <p><strong>But accept the accuracy tradeoff:</strong> ~1-2% worse than equivalent CNNs.</p> <hr> <h2 id="what-i-wish-id-done-differently">What I Wish I’d Done Differently</h2> <h3 id="1-test-with-data-augmentation">1. Test with Data Augmentation</h3> <p>All models were tested <strong>without data augmentation</strong>. This likely led to:</p> <ul> <li>Earlier overfitting</li> <li>Lower final accuracy</li> </ul> <p><strong>Data augmentation would have:</strong></p> <ul> <li>Reduced overfitting (especially for CNN-2, CNN-3)</li> <li>Potentially closed the gap between complex and simple models</li> </ul> <h3 id="2-use-learning-rate-schedules">2. Use Learning Rate Schedules</h3> <p>I used a fixed learning rate (0.001). <strong>Learning rate decay</strong> would have:</p> <ul> <li>Improved final accuracy</li> <li>Helped models converge better</li> </ul> <h3 id="3-test-batch-normalization">3. Test Batch Normalization</h3> <p>Modern CNNs use batch normalization between conv layers. This would have:</p> <ul> <li>Allowed deeper models without vanishing gradients</li> <li>Potentially made CNN-2 train faster</li> <li>Reduced overfitting</li> </ul> <p><strong>Without BN, comparing “deep” models isn’t entirely fair.</strong></p> <h3 id="4-measure-inference-time-not-just-training-time">4. Measure Inference Time, Not Just Training Time</h3> <p>I focused on training time, but for deployment, <strong>inference time</strong> matters more.</p> <p><strong>Hypothesis:</strong> FCNNs might be faster at inference (no dense layers), even if training is similar.</p> <hr> <h2 id="code--reproducibility">Code &amp; Reproducibility</h2> <p>All 7 architectures and training code in my <a href="https://github.com/LeonardoPaccianiMori/image-generation" rel="external nofollow noopener" target="_blank">Image Generation repository</a>:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">notebooks/cnn_comparison.ipynb</code> - All experiments</li> <li> <code class="language-plaintext highlighter-rouge">models/cnn_baseline.py</code> - CNN-1 implementation</li> <li> <code class="language-plaintext highlighter-rouge">models/fcnn_variants.py</code> - FCNN implementations</li> </ul> <p><strong>To reproduce:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/LeonardoPaccianiMori/image-generation
<span class="nb">cd </span>image-generation
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
jupyter notebook notebooks/cnn_comparison.ipynb
</code></pre></div></div> <hr> <h2 id="related-work">Related Work</h2> <p>This isn’t a new finding! Lots of research shows simpler models can match complex ones:</p> <ul> <li> <strong>“Do We Need Hundreds of Classifiers?”</strong> (<a href="https://jmlr.org/papers/v15/delgado14a.html" rel="external nofollow noopener" target="_blank">Fernández-Delgado et al., 2014</a>) - Random Forests beat many complex models</li> <li> <strong>“The Lottery Ticket Hypothesis”</strong> (<a href="https://arxiv.org/abs/1803.03635" rel="external nofollow noopener" target="_blank">Frankle &amp; Carbin, 2019</a>) - Large networks contain smaller subnetworks that perform equally well</li> <li> <strong>“Rethinking the Inception Architecture”</strong> (<a href="https://arxiv.org/abs/1512.00567" rel="external nofollow noopener" target="_blank">Szegedy et al., 2016</a>) - Factorized convolutions (smaller kernels) can be more efficient</li> </ul> <p><strong>My contribution:</strong> Systematic comparison on a specific task (MNIST) with practical insights for practitioners.</p> <hr> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li> <strong>Start with simple baselines</strong> - CNN-1 (98% in 12 min) beats complex models in efficiency</li> <li> <strong>More layers ≠ better results</strong> - CNN-3 had 3 FC layers but same accuracy as 1 FC layer</li> <li> <strong>Kernel size matters</strong> - FCNN-3 (5×5 kernels) beat FCNN-2 (more layers) with less training time</li> <li> <strong>Measure what matters</strong> - Accuracy alone doesn’t tell the story; consider training time, parameters, overfitting</li> <li> <strong>Complex models need careful tuning</strong> - Without data augmentation and proper regularization, they just overfit</li> <li> <strong>FCNNs trade accuracy for simplicity</strong> - 1-2% worse than CNNs, but fewer parameters</li> </ol> <p><strong>The meta-lesson:</strong> Don’t default to complexity. Simple models, properly trained, often win.</p> <p>Have you been surprised by a simple model outperforming a complex one? What’s your approach to choosing architectures? Let me know!</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Pinco Pallino. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>, forked from <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>